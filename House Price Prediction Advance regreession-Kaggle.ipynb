{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries:\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 81)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df.profile_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(['Alley','PoolQC','MiscFeature','Fence'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Data columns (total 77 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Id             1460 non-null   int64  \n",
      " 1   MSSubClass     1460 non-null   int64  \n",
      " 2   MSZoning       1460 non-null   object \n",
      " 3   LotFrontage    1201 non-null   float64\n",
      " 4   LotArea        1460 non-null   int64  \n",
      " 5   Street         1460 non-null   object \n",
      " 6   LotShape       1460 non-null   object \n",
      " 7   LandContour    1460 non-null   object \n",
      " 8   Utilities      1460 non-null   object \n",
      " 9   LotConfig      1460 non-null   object \n",
      " 10  LandSlope      1460 non-null   object \n",
      " 11  Neighborhood   1460 non-null   object \n",
      " 12  Condition1     1460 non-null   object \n",
      " 13  Condition2     1460 non-null   object \n",
      " 14  BldgType       1460 non-null   object \n",
      " 15  HouseStyle     1460 non-null   object \n",
      " 16  OverallQual    1460 non-null   int64  \n",
      " 17  OverallCond    1460 non-null   int64  \n",
      " 18  YearBuilt      1460 non-null   int64  \n",
      " 19  YearRemodAdd   1460 non-null   int64  \n",
      " 20  RoofStyle      1460 non-null   object \n",
      " 21  RoofMatl       1460 non-null   object \n",
      " 22  Exterior1st    1460 non-null   object \n",
      " 23  Exterior2nd    1460 non-null   object \n",
      " 24  MasVnrType     1452 non-null   object \n",
      " 25  MasVnrArea     1452 non-null   float64\n",
      " 26  ExterQual      1460 non-null   object \n",
      " 27  ExterCond      1460 non-null   object \n",
      " 28  Foundation     1460 non-null   object \n",
      " 29  BsmtQual       1423 non-null   object \n",
      " 30  BsmtCond       1423 non-null   object \n",
      " 31  BsmtExposure   1422 non-null   object \n",
      " 32  BsmtFinType1   1423 non-null   object \n",
      " 33  BsmtFinSF1     1460 non-null   int64  \n",
      " 34  BsmtFinType2   1422 non-null   object \n",
      " 35  BsmtFinSF2     1460 non-null   int64  \n",
      " 36  BsmtUnfSF      1460 non-null   int64  \n",
      " 37  TotalBsmtSF    1460 non-null   int64  \n",
      " 38  Heating        1460 non-null   object \n",
      " 39  HeatingQC      1460 non-null   object \n",
      " 40  CentralAir     1460 non-null   object \n",
      " 41  Electrical     1459 non-null   object \n",
      " 42  1stFlrSF       1460 non-null   int64  \n",
      " 43  2ndFlrSF       1460 non-null   int64  \n",
      " 44  LowQualFinSF   1460 non-null   int64  \n",
      " 45  GrLivArea      1460 non-null   int64  \n",
      " 46  BsmtFullBath   1460 non-null   int64  \n",
      " 47  BsmtHalfBath   1460 non-null   int64  \n",
      " 48  FullBath       1460 non-null   int64  \n",
      " 49  HalfBath       1460 non-null   int64  \n",
      " 50  BedroomAbvGr   1460 non-null   int64  \n",
      " 51  KitchenAbvGr   1460 non-null   int64  \n",
      " 52  KitchenQual    1460 non-null   object \n",
      " 53  TotRmsAbvGrd   1460 non-null   int64  \n",
      " 54  Functional     1460 non-null   object \n",
      " 55  Fireplaces     1460 non-null   int64  \n",
      " 56  FireplaceQu    770 non-null    object \n",
      " 57  GarageType     1379 non-null   object \n",
      " 58  GarageYrBlt    1379 non-null   float64\n",
      " 59  GarageFinish   1379 non-null   object \n",
      " 60  GarageCars     1460 non-null   int64  \n",
      " 61  GarageArea     1460 non-null   int64  \n",
      " 62  GarageQual     1379 non-null   object \n",
      " 63  GarageCond     1379 non-null   object \n",
      " 64  PavedDrive     1460 non-null   object \n",
      " 65  WoodDeckSF     1460 non-null   int64  \n",
      " 66  OpenPorchSF    1460 non-null   int64  \n",
      " 67  EnclosedPorch  1460 non-null   int64  \n",
      " 68  3SsnPorch      1460 non-null   int64  \n",
      " 69  ScreenPorch    1460 non-null   int64  \n",
      " 70  PoolArea       1460 non-null   int64  \n",
      " 71  MiscVal        1460 non-null   int64  \n",
      " 72  MoSold         1460 non-null   int64  \n",
      " 73  YrSold         1460 non-null   int64  \n",
      " 74  SaleType       1460 non-null   object \n",
      " 75  SaleCondition  1460 non-null   object \n",
      " 76  SalePrice      1460 non-null   int64  \n",
      "dtypes: float64(3), int64(35), object(39)\n",
      "memory usage: 878.4+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox â‰¥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"640\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x15e23bd5408>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.heatmap(train_df.isnull(),yticklabels=False,cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(['FireplaceQu'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['GarageYrBlt'] = train_df['GarageYrBlt'].fillna(train_df['GarageYrBlt'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['LotFrontage']=train_df['LotFrontage'].fillna(train_df['LotFrontage'].mean())\n",
    "train_df['BsmtCond']=train_df['BsmtCond'].fillna(train_df['BsmtCond'].mode()[0])\n",
    "train_df['BsmtQual']=train_df['BsmtQual'].fillna(train_df['BsmtQual'].mode()[0])\n",
    "train_df['GarageType']=train_df['GarageType'].fillna(train_df['GarageType'].mode()[0])\n",
    "train_df['GarageFinish']=train_df['GarageFinish'].fillna(train_df['GarageFinish'].mode()[0])\n",
    "train_df['GarageQual']=train_df['GarageQual'].fillna(train_df['GarageQual'].mode()[0])\n",
    "train_df['GarageCond']=train_df['GarageCond'].fillna(train_df['GarageCond'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['MasVnrType']=train_df['MasVnrType'].fillna(train_df['MasVnrType'].mode()[0])\n",
    "train_df['MasVnrArea']=train_df['MasVnrArea'].fillna(train_df['MasVnrArea'].mode()[0])\n",
    "train_df['BsmtExposure']=train_df['BsmtExposure'].fillna(train_df['BsmtExposure'].mode()[0])\n",
    "train_df['BsmtFinType2']=train_df['BsmtFinType2'].fillna(train_df['BsmtFinType2'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['BsmtFinType1']=train_df['BsmtFinType1'].fillna(train_df['BsmtFinType1'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Electrical']=train_df['Electrical'].fillna(train_df['Electrical'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 76)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['MSZoning','Street','LotShape','LandContour','Utilities','LotConfig','LandSlope','Neighborhood',\n",
    "         'Condition2','BldgType','Condition1','HouseStyle','SaleType',\n",
    "        'SaleCondition','ExterCond',\n",
    "         'ExterQual','Foundation','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2',\n",
    "        'RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','Heating','HeatingQC',\n",
    "         'CentralAir',\n",
    "         'Electrical','KitchenQual','Functional',\n",
    "         'FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond','PavedDrive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_onehot_multcols(multcolumns):\n",
    "    df_final=final_df\n",
    "    i=0\n",
    "    for fields in multcolumns:\n",
    "        \n",
    "        print(fields)\n",
    "        df1=pd.get_dummies(final_df[fields],drop_first=True)\n",
    "        \n",
    "        final_df.drop([fields],axis=1,inplace=True)\n",
    "        if i==0:\n",
    "            df_final=df1.copy()\n",
    "        else:\n",
    "            \n",
    "            df_final=pd.concat([df_final,df1],axis=1)\n",
    "        i=i+1\n",
    "       \n",
    "        \n",
    "    df_final=pd.concat([final_df,df_final],axis=1)\n",
    "        \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df=train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combine Test Data \n",
    "\n",
    "test_df=pd.read_csv('formulatedtest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df=pd.concat([train_df,test_df],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       208500.0\n",
       "1       181500.0\n",
       "2       223500.0\n",
       "3       140000.0\n",
       "4       250000.0\n",
       "          ...   \n",
       "1454         NaN\n",
       "1455         NaN\n",
       "1456         NaN\n",
       "1457         NaN\n",
       "1458         NaN\n",
       "Name: SalePrice, Length: 2919, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 77)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSZoning\n",
      "Street\n",
      "LotShape\n",
      "LandContour\n",
      "Utilities\n",
      "LotConfig\n",
      "LandSlope\n",
      "Neighborhood\n",
      "Condition2\n",
      "BldgType\n",
      "Condition1\n",
      "HouseStyle\n",
      "SaleType\n",
      "SaleCondition\n",
      "ExterCond\n",
      "ExterQual\n",
      "Foundation\n",
      "BsmtQual\n",
      "BsmtCond\n",
      "BsmtExposure\n",
      "BsmtFinType1\n",
      "BsmtFinType2\n",
      "RoofStyle\n",
      "RoofMatl\n",
      "Exterior1st\n",
      "Exterior2nd\n",
      "MasVnrType\n",
      "Heating\n",
      "HeatingQC\n",
      "CentralAir\n",
      "Electrical\n",
      "KitchenQual\n",
      "Functional\n",
      "FireplaceQu\n",
      "GarageType\n",
      "GarageFinish\n",
      "GarageQual\n",
      "GarageCond\n",
      "PavedDrive\n"
     ]
    }
   ],
   "source": [
    "final_df=category_onehot_multcols(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 238)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df =final_df.loc[:,~final_df.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 178)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>...</th>\n",
       "      <th>Min1</th>\n",
       "      <th>Min2</th>\n",
       "      <th>Typ</th>\n",
       "      <th>Attchd</th>\n",
       "      <th>Basment</th>\n",
       "      <th>BuiltIn</th>\n",
       "      <th>CarPort</th>\n",
       "      <th>Detchd</th>\n",
       "      <th>RFn</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>196.0</td>\n",
       "      <td>706.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>162.0</td>\n",
       "      <td>486.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>350.0</td>\n",
       "      <td>655.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2915</td>\n",
       "      <td>160</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1936</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1970</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>2916</td>\n",
       "      <td>160</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1894</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1970</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2917</td>\n",
       "      <td>20</td>\n",
       "      <td>160.0</td>\n",
       "      <td>20000</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1960</td>\n",
       "      <td>1996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1224.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2918</td>\n",
       "      <td>85</td>\n",
       "      <td>62.0</td>\n",
       "      <td>10441</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1992</td>\n",
       "      <td>1992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>337.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2919</td>\n",
       "      <td>60</td>\n",
       "      <td>74.0</td>\n",
       "      <td>9627</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1993</td>\n",
       "      <td>1994</td>\n",
       "      <td>94.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2919 rows Ã— 178 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id  MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  \\\n",
       "0        1          60         65.0     8450            7            5   \n",
       "1        2          20         80.0     9600            6            8   \n",
       "2        3          60         68.0    11250            7            5   \n",
       "3        4          70         60.0     9550            7            5   \n",
       "4        5          60         84.0    14260            8            5   \n",
       "...    ...         ...          ...      ...          ...          ...   \n",
       "1454  2915         160         21.0     1936            4            7   \n",
       "1455  2916         160         21.0     1894            4            5   \n",
       "1456  2917          20        160.0    20000            5            7   \n",
       "1457  2918          85         62.0    10441            5            5   \n",
       "1458  2919          60         74.0     9627            7            5   \n",
       "\n",
       "      YearBuilt  YearRemodAdd  MasVnrArea  BsmtFinSF1  ...  Min1  Min2  Typ  \\\n",
       "0          2003          2003       196.0       706.0  ...     0     0    1   \n",
       "1          1976          1976         0.0       978.0  ...     0     0    1   \n",
       "2          2001          2002       162.0       486.0  ...     0     0    1   \n",
       "3          1915          1970         0.0       216.0  ...     0     0    1   \n",
       "4          2000          2000       350.0       655.0  ...     0     0    1   \n",
       "...         ...           ...         ...         ...  ...   ...   ...  ...   \n",
       "1454       1970          1970         0.0         0.0  ...     0     0    1   \n",
       "1455       1970          1970         0.0       252.0  ...     0     0    1   \n",
       "1456       1960          1996         0.0      1224.0  ...     0     0    1   \n",
       "1457       1992          1992         0.0       337.0  ...     0     0    1   \n",
       "1458       1993          1994        94.0       758.0  ...     0     0    1   \n",
       "\n",
       "      Attchd  Basment  BuiltIn  CarPort  Detchd  RFn  P  \n",
       "0          1        0        0        0       0    1  0  \n",
       "1          1        0        0        0       0    1  0  \n",
       "2          1        0        0        0       0    1  0  \n",
       "3          0        0        0        0       1    0  0  \n",
       "4          1        0        0        0       0    1  0  \n",
       "...      ...      ...      ...      ...     ...  ... ..  \n",
       "1454       1        0        0        0       0    0  0  \n",
       "1455       0        0        0        1       0    0  0  \n",
       "1456       0        0        0        0       1    0  0  \n",
       "1457       1        0        0        0       0    0  0  \n",
       "1458       1        0        0        0       0    0  0  \n",
       "\n",
       "[2919 rows x 178 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Train=final_df.iloc[:1460,:]\n",
    "df_Test=final_df.iloc[1460:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 178)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1461\n",
       "1       1462\n",
       "2       1463\n",
       "3       1464\n",
       "4       1465\n",
       "        ... \n",
       "1454    2915\n",
       "1455    2916\n",
       "1456    2917\n",
       "1457    2918\n",
       "1458    2919\n",
       "Name: Id, Length: 1459, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Test['Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Test = df_Test.drop(['SalePrice'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=df_Train.drop(['SalePrice'],axis=1)\n",
    "y_train=df_Train['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "regressor=xgboost.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "booster=['gbtree','gblinear']\n",
    "base_score=[0.25,0.5,0.75,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [100, 500, 900, 1100, 1500]\n",
    "max_depth = [2, 3, 5, 10, 15]\n",
    "booster=['gbtree','gblinear']\n",
    "learning_rate=[0.05,0.1,0.15,0.20]\n",
    "min_child_weight=[1,2,3,4]\n",
    "\n",
    "# Define the grid of hyperparameters to search\n",
    "hyperparameter_grid = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_depth':max_depth,\n",
    "    'learning_rate':learning_rate,\n",
    "    'min_child_weight':min_child_weight,\n",
    "    'booster':booster,\n",
    "    'base_score':base_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the random search with 4-fold cross validation\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "random_cv = RandomizedSearchCV(estimator=regressor,\n",
    "            param_distributions=hyperparameter_grid,\n",
    "            cv=5, n_iter=50,\n",
    "            scoring = 'neg_mean_absolute_error',n_jobs = 4,\n",
    "            verbose = 5, \n",
    "            return_train_score = True,\n",
    "            random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:   15.1s\n",
      "[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=4)]: Done 250 out of 250 | elapsed:  9.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:23:27] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score=nan,\n",
       "                   estimator=XGBRegressor(base_score=0.5, booster='gbtree',\n",
       "                                          colsample_bylevel=1,\n",
       "                                          colsample_bynode=1,\n",
       "                                          colsample_bytree=1, gamma=0,\n",
       "                                          importance_type='gain',\n",
       "                                          learning_rate=0.1, max_delta_step=0,\n",
       "                                          max_depth=3, min_child_weight=1,\n",
       "                                          missing=None, n_estimators=100,\n",
       "                                          n_jobs=1, nthread=None,\n",
       "                                          objective='reg:linear',\n",
       "                                          random_state=0, reg_alpha=...\n",
       "                   iid='deprecated', n_iter=50, n_jobs=4,\n",
       "                   param_distributions={'base_score': [0.25, 0.5, 0.75, 1],\n",
       "                                        'booster': ['gbtree', 'gblinear'],\n",
       "                                        'learning_rate': [0.05, 0.1, 0.15, 0.2],\n",
       "                                        'max_depth': [2, 3, 5, 10, 15],\n",
       "                                        'min_child_weight': [1, 2, 3, 4],\n",
       "                                        'n_estimators': [100, 500, 900, 1100,\n",
       "                                                         1500]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=True, scoring='neg_mean_absolute_error',\n",
       "                   verbose=5)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_cv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.25, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=2, min_child_weight=1, missing=None, n_estimators=900,\n",
       "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "             silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor=xgboost.XGBRegressor(base_score=0.25, booster='gbtree', colsample_bylevel=1,\n",
    "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
    "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
    "             max_depth=2, min_child_weight=1, missing=None, n_estimators=900,\n",
    "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
    "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "             silent=None, subsample=1, verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:24:59] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.25, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=2, min_child_weight=1, missing=None, n_estimators=900,\n",
       "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "             silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'finalized_model.pkl'\n",
    "pickle.dump(regressor, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 177)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=regressor.predict(df_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([123435.88, 161721.92, 184340.3 , ..., 179532.27, 117466.28,\n",
       "       233489.22], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=pd.DataFrame(y_pred)\n",
    "sub_df=pd.read_csv('sample_submission.csv')\n",
    "datasets=pd.concat([sub_df['Id'],pred],axis=1)\n",
    "datasets.columns=['Id','SalePrice']\n",
    "datasets.to_csv('sample_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.columns=['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df=df_Train['SalePrice'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.column=['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hgoya\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3994: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "df_Train.drop(['SalePrice'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Train=pd.concat([df_Train,temp_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Test=pd.concat([df_Test,pred],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Train=pd.concat([df_Train,df_Test],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=df_Train.drop(['SalePrice'],axis=1)\n",
    "y_train=df_Train['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hgoya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=177, units=50, kernel_initializer=\"he_uniform\")`\n",
      "  del sys.path[0]\n",
      "C:\\Users\\hgoya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=25, kernel_initializer=\"he_uniform\")`\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\hgoya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=50, kernel_initializer=\"he_uniform\")`\n",
      "C:\\Users\\hgoya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=1, kernel_initializer=\"he_uniform\")`\n",
      "C:\\Users\\hgoya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2335 samples, validate on 584 samples\n",
      "Epoch 1/1000\n",
      "2335/2335 [==============================] - 1s 641us/step - loss: 104183.0723 - val_loss: 54466.3087\n",
      "Epoch 2/1000\n",
      "2335/2335 [==============================] - 0s 156us/step - loss: 61849.9811 - val_loss: 49536.4983\n",
      "Epoch 3/1000\n",
      "2335/2335 [==============================] - 0s 152us/step - loss: 56024.5412 - val_loss: 48611.3584\n",
      "Epoch 4/1000\n",
      "2335/2335 [==============================] - 0s 139us/step - loss: 50311.5245 - val_loss: 40092.8669\n",
      "Epoch 5/1000\n",
      "2335/2335 [==============================] - 0s 165us/step - loss: 44706.8359 - val_loss: 34679.8861\n",
      "Epoch 6/1000\n",
      "2335/2335 [==============================] - 0s 151us/step - loss: 39860.2251 - val_loss: 32631.1197\n",
      "Epoch 7/1000\n",
      "2335/2335 [==============================] - 0s 150us/step - loss: 37674.8811 - val_loss: 32188.8379\n",
      "Epoch 8/1000\n",
      "2335/2335 [==============================] - 0s 166us/step - loss: 36567.3139 - val_loss: 32009.4685\n",
      "Epoch 9/1000\n",
      "2335/2335 [==============================] - 0s 169us/step - loss: 35787.7077 - val_loss: 32232.3840\n",
      "Epoch 10/1000\n",
      "2335/2335 [==============================] - 0s 153us/step - loss: 35474.8811 - val_loss: 31897.3932\n",
      "Epoch 11/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 35589.9588 - val_loss: 32310.6858\n",
      "Epoch 12/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 35003.7892 - val_loss: 33160.7588\n",
      "Epoch 13/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 34864.0594 - val_loss: 31521.7614\n",
      "Epoch 14/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 34819.3196 - val_loss: 31689.8235\n",
      "Epoch 15/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 34781.8084 - val_loss: 31991.4409\n",
      "Epoch 16/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 34458.3402 - val_loss: 32759.1917\n",
      "Epoch 17/1000\n",
      "2335/2335 [==============================] - 0s 191us/step - loss: 34868.5206 - val_loss: 31871.8551\n",
      "Epoch 18/1000\n",
      "2335/2335 [==============================] - 0s 166us/step - loss: 34808.1014 - val_loss: 32004.8923\n",
      "Epoch 19/1000\n",
      "2335/2335 [==============================] - 0s 152us/step - loss: 34646.8492 - val_loss: 31265.8749\n",
      "Epoch 20/1000\n",
      "2335/2335 [==============================] - 0s 156us/step - loss: 34502.2735 - val_loss: 32337.4087\n",
      "Epoch 21/1000\n",
      "2335/2335 [==============================] - 0s 159us/step - loss: 34483.8001 - val_loss: 31227.9271\n",
      "Epoch 22/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 34345.8173 - val_loss: 31284.3887\n",
      "Epoch 23/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 34116.1389 - val_loss: 31134.1123\n",
      "Epoch 24/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 34264.6460 - val_loss: 31759.8064\n",
      "Epoch 25/1000\n",
      "2335/2335 [==============================] - 0s 145us/step - loss: 34319.9300 - val_loss: 33124.7204\n",
      "Epoch 26/1000\n",
      "2335/2335 [==============================] - 0s 166us/step - loss: 34129.3804 - val_loss: 31357.2231\n",
      "Epoch 27/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 34188.5955 - val_loss: 30930.2618\n",
      "Epoch 28/1000\n",
      "2335/2335 [==============================] - 0s 151us/step - loss: 34350.6336 - val_loss: 30886.7480\n",
      "Epoch 29/1000\n",
      "2335/2335 [==============================] - 0s 167us/step - loss: 33936.2093 - val_loss: 30954.2456\n",
      "Epoch 30/1000\n",
      "2335/2335 [==============================] - 0s 173us/step - loss: 33945.0882 - val_loss: 33145.5438\n",
      "Epoch 31/1000\n",
      "2335/2335 [==============================] - 0s 153us/step - loss: 33957.4019 - val_loss: 31570.3124\n",
      "Epoch 32/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 33671.7042 - val_loss: 33047.3942\n",
      "Epoch 33/1000\n",
      "2335/2335 [==============================] - 0s 144us/step - loss: 34105.9424 - val_loss: 30648.0751\n",
      "Epoch 34/1000\n",
      "2335/2335 [==============================] - 0s 160us/step - loss: 33767.6319 - val_loss: 31890.4784\n",
      "Epoch 35/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 33543.0939 - val_loss: 31067.3479\n",
      "Epoch 36/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 33541.7558 - val_loss: 33531.3190\n",
      "Epoch 37/1000\n",
      "2335/2335 [==============================] - 0s 150us/step - loss: 33434.1849 - val_loss: 30524.2961\n",
      "Epoch 38/1000\n",
      "2335/2335 [==============================] - 0s 162us/step - loss: 33376.9647 - val_loss: 30427.2226\n",
      "Epoch 39/1000\n",
      "2335/2335 [==============================] - 0s 161us/step - loss: 33202.5518 - val_loss: 30353.1868\n",
      "Epoch 40/1000\n",
      "2335/2335 [==============================] - 0s 155us/step - loss: 33169.7937 - val_loss: 30829.9121\n",
      "Epoch 41/1000\n",
      "2335/2335 [==============================] - 0s 156us/step - loss: 33264.5457 - val_loss: 32275.5277\n",
      "Epoch 42/1000\n",
      "2335/2335 [==============================] - 0s 159us/step - loss: 33213.4550 - val_loss: 31324.3230\n",
      "Epoch 43/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 33013.4152 - val_loss: 30584.8485\n",
      "Epoch 44/1000\n",
      "2335/2335 [==============================] - 0s 163us/step - loss: 33139.8437 - val_loss: 30495.6949\n",
      "Epoch 45/1000\n",
      "2335/2335 [==============================] - 0s 165us/step - loss: 32757.8748 - val_loss: 32541.1264\n",
      "Epoch 46/1000\n",
      "2335/2335 [==============================] - 1s 219us/step - loss: 32740.1399 - val_loss: 30213.3276\n",
      "Epoch 47/1000\n",
      "2335/2335 [==============================] - 0s 167us/step - loss: 32855.6982 - val_loss: 31037.4600\n",
      "Epoch 48/1000\n",
      "2335/2335 [==============================] - 0s 140us/step - loss: 33020.4278 - val_loss: 29869.4965\n",
      "Epoch 49/1000\n",
      "2335/2335 [==============================] - 0s 174us/step - loss: 32891.1625 - val_loss: 29808.6863\n",
      "Epoch 50/1000\n",
      "2335/2335 [==============================] - 0s 146us/step - loss: 32278.8139 - val_loss: 29871.1683\n",
      "Epoch 51/1000\n",
      "2335/2335 [==============================] - 0s 189us/step - loss: 32261.0143 - val_loss: 29946.8220\n",
      "Epoch 52/1000\n",
      "2335/2335 [==============================] - 0s 189us/step - loss: 32531.2368 - val_loss: 30856.9417\n",
      "Epoch 53/1000\n",
      "2335/2335 [==============================] - 0s 160us/step - loss: 32075.1799 - val_loss: 29780.4728\n",
      "Epoch 54/1000\n",
      "2335/2335 [==============================] - 0s 166us/step - loss: 32053.1857 - val_loss: 29862.9988\n",
      "Epoch 55/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 31958.6518 - val_loss: 30887.0884\n",
      "Epoch 56/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 31696.1929 - val_loss: 31701.4194\n",
      "Epoch 57/1000\n",
      "2335/2335 [==============================] - 0s 190us/step - loss: 31923.1593 - val_loss: 29199.1692\n",
      "Epoch 58/1000\n",
      "2335/2335 [==============================] - 0s 153us/step - loss: 31675.8300 - val_loss: 29197.2193\n",
      "Epoch 59/1000\n",
      "2335/2335 [==============================] - 0s 153us/step - loss: 31766.3011 - val_loss: 31176.2568\n",
      "Epoch 60/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 32178.8501 - val_loss: 29441.9081\n",
      "Epoch 61/1000\n",
      "2335/2335 [==============================] - 0s 152us/step - loss: 31502.4173 - val_loss: 29172.5074\n",
      "Epoch 62/1000\n",
      "2335/2335 [==============================] - 0s 152us/step - loss: 31697.2985 - val_loss: 29388.6726\n",
      "Epoch 63/1000\n",
      "2335/2335 [==============================] - 0s 152us/step - loss: 31498.0888 - val_loss: 29118.6324\n",
      "Epoch 64/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 31490.5159 - val_loss: 30319.5098\n",
      "Epoch 65/1000\n",
      "2335/2335 [==============================] - 0s 136us/step - loss: 31553.5156 - val_loss: 29010.6166\n",
      "Epoch 66/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 31242.3885 - val_loss: 28942.4872\n",
      "Epoch 67/1000\n",
      "2335/2335 [==============================] - 0s 162us/step - loss: 31626.4388 - val_loss: 28825.2993\n",
      "Epoch 68/1000\n",
      "2335/2335 [==============================] - 0s 139us/step - loss: 31046.2271 - val_loss: 28816.7814\n",
      "Epoch 69/1000\n",
      "2335/2335 [==============================] - 0s 186us/step - loss: 31412.6669 - val_loss: 28798.9688\n",
      "Epoch 70/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 30865.5657 - val_loss: 29040.6880\n",
      "Epoch 71/1000\n",
      "2335/2335 [==============================] - 0s 194us/step - loss: 31131.6158 - val_loss: 28442.0848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/1000\n",
      "2335/2335 [==============================] - 0s 189us/step - loss: 31093.7067 - val_loss: 29046.1074\n",
      "Epoch 73/1000\n",
      "2335/2335 [==============================] - 0s 164us/step - loss: 30851.1488 - val_loss: 28251.6647\n",
      "Epoch 74/1000\n",
      "2335/2335 [==============================] - 0s 144us/step - loss: 30776.8812 - val_loss: 28756.6448\n",
      "Epoch 75/1000\n",
      "2335/2335 [==============================] - 0s 159us/step - loss: 30426.2689 - val_loss: 29348.2615\n",
      "Epoch 76/1000\n",
      "2335/2335 [==============================] - 0s 133us/step - loss: 30889.9645 - val_loss: 28751.2761\n",
      "Epoch 77/1000\n",
      "2335/2335 [==============================] - ETA: 0s - loss: 30469.711 - 0s 150us/step - loss: 30539.3158 - val_loss: 28474.6870\n",
      "Epoch 78/1000\n",
      "2335/2335 [==============================] - 0s 150us/step - loss: 30270.4090 - val_loss: 29249.7280\n",
      "Epoch 79/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 30076.6342 - val_loss: 27789.7539\n",
      "Epoch 80/1000\n",
      "2335/2335 [==============================] - 0s 153us/step - loss: 30235.3016 - val_loss: 29185.7835\n",
      "Epoch 81/1000\n",
      "2335/2335 [==============================] - 0s 139us/step - loss: 30394.0249 - val_loss: 27791.9764\n",
      "Epoch 82/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 30056.4102 - val_loss: 27755.2479\n",
      "Epoch 83/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 30158.6918 - val_loss: 27868.6286\n",
      "Epoch 84/1000\n",
      "2335/2335 [==============================] - 0s 141us/step - loss: 29945.5806 - val_loss: 27759.5513\n",
      "Epoch 85/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 29910.1878 - val_loss: 31216.1786\n",
      "Epoch 86/1000\n",
      "2335/2335 [==============================] - 0s 176us/step - loss: 30006.4158 - val_loss: 27576.1723\n",
      "Epoch 87/1000\n",
      "2335/2335 [==============================] - 0s 138us/step - loss: 29774.9904 - val_loss: 27262.7015\n",
      "Epoch 88/1000\n",
      "2335/2335 [==============================] - 0s 165us/step - loss: 29742.9267 - val_loss: 27317.7732\n",
      "Epoch 89/1000\n",
      "2335/2335 [==============================] - 0s 167us/step - loss: 29421.7220 - val_loss: 27264.7665\n",
      "Epoch 90/1000\n",
      "2335/2335 [==============================] - 0s 163us/step - loss: 29284.1296 - val_loss: 28048.5563\n",
      "Epoch 91/1000\n",
      "2335/2335 [==============================] - 0s 167us/step - loss: 29267.6400 - val_loss: 27256.7356\n",
      "Epoch 92/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 29599.6094 - val_loss: 27642.8809\n",
      "Epoch 93/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 29465.4316 - val_loss: 26783.2045\n",
      "Epoch 94/1000\n",
      "2335/2335 [==============================] - 0s 170us/step - loss: 29145.1097 - val_loss: 26574.6305\n",
      "Epoch 95/1000\n",
      "2335/2335 [==============================] - 0s 137us/step - loss: 29304.4711 - val_loss: 26623.5783\n",
      "Epoch 96/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 28798.8244 - val_loss: 26458.1664\n",
      "Epoch 97/1000\n",
      "2335/2335 [==============================] - 0s 170us/step - loss: 28814.8267 - val_loss: 26837.3870\n",
      "Epoch 98/1000\n",
      "2335/2335 [==============================] - 0s 153us/step - loss: 28825.5805 - val_loss: 26395.0728\n",
      "Epoch 99/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 28807.9863 - val_loss: 26272.4774\n",
      "Epoch 100/1000\n",
      "2335/2335 [==============================] - 0s 174us/step - loss: 28479.2851 - val_loss: 26067.7650\n",
      "Epoch 101/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 28396.5831 - val_loss: 26337.8961\n",
      "Epoch 102/1000\n",
      "2335/2335 [==============================] - 0s 150us/step - loss: 28215.5794 - val_loss: 30140.6350\n",
      "Epoch 103/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 28473.2452 - val_loss: 29378.2748\n",
      "Epoch 104/1000\n",
      "2335/2335 [==============================] - 0s 165us/step - loss: 28409.3843 - val_loss: 25546.4150\n",
      "Epoch 105/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 28152.9116 - val_loss: 25754.4019\n",
      "Epoch 106/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 28103.0843 - val_loss: 25365.8568\n",
      "Epoch 107/1000\n",
      "2335/2335 [==============================] - 0s 161us/step - loss: 28100.2884 - val_loss: 26209.3585\n",
      "Epoch 108/1000\n",
      "2335/2335 [==============================] - 0s 151us/step - loss: 27979.4263 - val_loss: 26041.9545\n",
      "Epoch 109/1000\n",
      "2335/2335 [==============================] - 0s 147us/step - loss: 28165.6025 - val_loss: 25574.5620\n",
      "Epoch 110/1000\n",
      "2335/2335 [==============================] - 0s 176us/step - loss: 27937.7441 - val_loss: 25271.4352\n",
      "Epoch 111/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 27557.1886 - val_loss: 25291.8351\n",
      "Epoch 112/1000\n",
      "2335/2335 [==============================] - 0s 159us/step - loss: 27334.7724 - val_loss: 25034.0302\n",
      "Epoch 113/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 27142.2302 - val_loss: 25740.1831\n",
      "Epoch 114/1000\n",
      "2335/2335 [==============================] - 0s 151us/step - loss: 27716.8542 - val_loss: 24985.0446\n",
      "Epoch 115/1000\n",
      "2335/2335 [==============================] - 0s 155us/step - loss: 27373.2465 - val_loss: 24600.4785\n",
      "Epoch 116/1000\n",
      "2335/2335 [==============================] - 0s 156us/step - loss: 27190.0267 - val_loss: 24328.3447\n",
      "Epoch 117/1000\n",
      "2335/2335 [==============================] - 0s 165us/step - loss: 26953.6308 - val_loss: 27625.2964\n",
      "Epoch 118/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 26778.3602 - val_loss: 24820.5788\n",
      "Epoch 119/1000\n",
      "2335/2335 [==============================] - 0s 161us/step - loss: 26607.5022 - val_loss: 24005.8082\n",
      "Epoch 120/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 26625.2669 - val_loss: 24875.2819\n",
      "Epoch 121/1000\n",
      "2335/2335 [==============================] - 0s 156us/step - loss: 26980.2990 - val_loss: 25541.9288\n",
      "Epoch 122/1000\n",
      "2335/2335 [==============================] - 0s 164us/step - loss: 26638.3741 - val_loss: 23425.9208\n",
      "Epoch 123/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 26510.1365 - val_loss: 24042.3803\n",
      "Epoch 124/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 26390.4832 - val_loss: 25519.3004\n",
      "Epoch 125/1000\n",
      "2335/2335 [==============================] - 0s 164us/step - loss: 26485.2579 - val_loss: 23212.3555\n",
      "Epoch 126/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 26584.5749 - val_loss: 22840.8634\n",
      "Epoch 127/1000\n",
      "2335/2335 [==============================] - 0s 150us/step - loss: 26590.5920 - val_loss: 24714.5606\n",
      "Epoch 128/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 25988.5893 - val_loss: 23531.6735\n",
      "Epoch 129/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 26144.2831 - val_loss: 22762.8418\n",
      "Epoch 130/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 26150.4977 - val_loss: 23016.3156\n",
      "Epoch 131/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 25853.8270 - val_loss: 24419.5767\n",
      "Epoch 132/1000\n",
      "2335/2335 [==============================] - 0s 163us/step - loss: 25645.3375 - val_loss: 22607.3120\n",
      "Epoch 133/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 25772.1541 - val_loss: 23156.2620\n",
      "Epoch 134/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 25533.0463 - val_loss: 23866.9396\n",
      "Epoch 135/1000\n",
      "2335/2335 [==============================] - 0s 164us/step - loss: 25310.3748 - val_loss: 24534.9334\n",
      "Epoch 136/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 25835.1606 - val_loss: 24243.6729\n",
      "Epoch 137/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 25443.3149 - val_loss: 25084.4635\n",
      "Epoch 138/1000\n",
      "2335/2335 [==============================] - 0s 161us/step - loss: 25514.5017 - val_loss: 22011.5459\n",
      "Epoch 139/1000\n",
      "2335/2335 [==============================] - 0s 155us/step - loss: 25114.4336 - val_loss: 22674.6394\n",
      "Epoch 140/1000\n",
      "2335/2335 [==============================] - 0s 144us/step - loss: 25147.5784 - val_loss: 21575.1336\n",
      "Epoch 141/1000\n",
      "2335/2335 [==============================] - 0s 186us/step - loss: 24918.5299 - val_loss: 21371.8374\n",
      "Epoch 142/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2335/2335 [==============================] - 0s 170us/step - loss: 24640.1355 - val_loss: 21646.1473\n",
      "Epoch 143/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 24922.3187 - val_loss: 21805.9809\n",
      "Epoch 144/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 25004.2701 - val_loss: 22094.5376\n",
      "Epoch 145/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 24629.8181 - val_loss: 21188.5441\n",
      "Epoch 146/1000\n",
      "2335/2335 [==============================] - 0s 136us/step - loss: 25264.8591 - val_loss: 21253.0459\n",
      "Epoch 147/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 24546.6958 - val_loss: 24937.2063\n",
      "Epoch 148/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 24922.0378 - val_loss: 20964.9552\n",
      "Epoch 149/1000\n",
      "2335/2335 [==============================] - 0s 147us/step - loss: 24487.0839 - val_loss: 21122.8641\n",
      "Epoch 150/1000\n",
      "2335/2335 [==============================] - 0s 144us/step - loss: 24615.8193 - val_loss: 20517.0244\n",
      "Epoch 151/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 24786.4924 - val_loss: 21297.3701\n",
      "Epoch 152/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 24746.0945 - val_loss: 26142.9023\n",
      "Epoch 153/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 24550.7495 - val_loss: 20706.0518\n",
      "Epoch 154/1000\n",
      "2335/2335 [==============================] - 0s 164us/step - loss: 24129.0406 - val_loss: 21841.4246\n",
      "Epoch 155/1000\n",
      "2335/2335 [==============================] - 0s 136us/step - loss: 24588.1870 - val_loss: 20532.0043\n",
      "Epoch 156/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 24325.6616 - val_loss: 19973.1068\n",
      "Epoch 157/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 24077.1825 - val_loss: 20676.1789\n",
      "Epoch 158/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 24766.0903 - val_loss: 20425.9552\n",
      "Epoch 159/1000\n",
      "2335/2335 [==============================] - 0s 152us/step - loss: 24140.6721 - val_loss: 19987.4325\n",
      "Epoch 160/1000\n",
      "2335/2335 [==============================] - 0s 153us/step - loss: 24225.0948 - val_loss: 20602.5185\n",
      "Epoch 161/1000\n",
      "2335/2335 [==============================] - 0s 148us/step - loss: 23889.6940 - val_loss: 19941.9906\n",
      "Epoch 162/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 23914.9995 - val_loss: 22554.1676\n",
      "Epoch 163/1000\n",
      "2335/2335 [==============================] - 0s 153us/step - loss: 23323.3506 - val_loss: 20483.1703\n",
      "Epoch 164/1000\n",
      "2335/2335 [==============================] - 0s 156us/step - loss: 23861.8295 - val_loss: 22186.7823\n",
      "Epoch 165/1000\n",
      "2335/2335 [==============================] - 0s 137us/step - loss: 23634.0544 - val_loss: 19939.9958\n",
      "Epoch 166/1000\n",
      "2335/2335 [==============================] - 0s 159us/step - loss: 23651.2112 - val_loss: 20382.9671\n",
      "Epoch 167/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 24181.7444 - val_loss: 19551.3765\n",
      "Epoch 168/1000\n",
      "2335/2335 [==============================] - 0s 136us/step - loss: 24290.2272 - val_loss: 19720.3801\n",
      "Epoch 169/1000\n",
      "2335/2335 [==============================] - 0s 162us/step - loss: 23891.0350 - val_loss: 20152.0577\n",
      "Epoch 170/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 23764.7100 - val_loss: 21817.7179\n",
      "Epoch 171/1000\n",
      "2335/2335 [==============================] - 0s 137us/step - loss: 23646.0082 - val_loss: 26901.9632\n",
      "Epoch 172/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 23442.7325 - val_loss: 19349.2374\n",
      "Epoch 173/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 23950.4510 - val_loss: 20441.4815\n",
      "Epoch 174/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 23727.5629 - val_loss: 19351.2222\n",
      "Epoch 175/1000\n",
      "2335/2335 [==============================] - 0s 164us/step - loss: 23403.6470 - val_loss: 19818.6794\n",
      "Epoch 176/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 23613.8199 - val_loss: 20149.4236\n",
      "Epoch 177/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 23573.1886 - val_loss: 26737.4317\n",
      "Epoch 178/1000\n",
      "2335/2335 [==============================] - 0s 151us/step - loss: 23313.4913 - val_loss: 19004.9543\n",
      "Epoch 179/1000\n",
      "2335/2335 [==============================] - 0s 155us/step - loss: 23465.4484 - val_loss: 19234.1756\n",
      "Epoch 180/1000\n",
      "2335/2335 [==============================] - 0s 153us/step - loss: 23726.9993 - val_loss: 22394.0038\n",
      "Epoch 181/1000\n",
      "2335/2335 [==============================] - 0s 139us/step - loss: 23368.7023 - val_loss: 19389.5978\n",
      "Epoch 182/1000\n",
      "2335/2335 [==============================] - 0s 150us/step - loss: 23697.8498 - val_loss: 20931.1341\n",
      "Epoch 183/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 23089.7004 - val_loss: 21837.5849\n",
      "Epoch 184/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 23172.8264 - val_loss: 20691.5475\n",
      "Epoch 185/1000\n",
      "2335/2335 [==============================] - 0s 165us/step - loss: 23967.7125 - val_loss: 19074.1603\n",
      "Epoch 186/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 23647.0283 - val_loss: 19148.1159\n",
      "Epoch 187/1000\n",
      "2335/2335 [==============================] - 0s 141us/step - loss: 23352.7323 - val_loss: 19357.7907\n",
      "Epoch 188/1000\n",
      "2335/2335 [==============================] - 0s 150us/step - loss: 23138.5134 - val_loss: 22473.3315\n",
      "Epoch 189/1000\n",
      "2335/2335 [==============================] - 0s 161us/step - loss: 23173.8150 - val_loss: 18807.0461\n",
      "Epoch 190/1000\n",
      "2335/2335 [==============================] - 0s 139us/step - loss: 23191.8997 - val_loss: 20287.5281\n",
      "Epoch 191/1000\n",
      "2335/2335 [==============================] - 0s 156us/step - loss: 22923.6142 - val_loss: 20065.5515\n",
      "Epoch 192/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 23230.7361 - val_loss: 19907.5366\n",
      "Epoch 193/1000\n",
      "2335/2335 [==============================] - 0s 139us/step - loss: 22646.1771 - val_loss: 22642.6643\n",
      "Epoch 194/1000\n",
      "2335/2335 [==============================] - 0s 156us/step - loss: 22867.3225 - val_loss: 20222.3545\n",
      "Epoch 195/1000\n",
      "2335/2335 [==============================] - 0s 152us/step - loss: 22745.6842 - val_loss: 19851.2195\n",
      "Epoch 196/1000\n",
      "2335/2335 [==============================] - 0s 140us/step - loss: 23096.2174 - val_loss: 18155.1454\n",
      "Epoch 197/1000\n",
      "2335/2335 [==============================] - 0s 137us/step - loss: 23073.3037 - val_loss: 18436.6687\n",
      "Epoch 198/1000\n",
      "2335/2335 [==============================] - 0s 164us/step - loss: 22744.3325 - val_loss: 18201.6971\n",
      "Epoch 199/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 22459.3647 - val_loss: 18987.7944\n",
      "Epoch 200/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 22763.7051 - val_loss: 18278.3066\n",
      "Epoch 201/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 22547.5692 - val_loss: 19622.7347\n",
      "Epoch 202/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 22470.8013 - val_loss: 19814.6251\n",
      "Epoch 203/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 22853.7660 - val_loss: 19315.4106\n",
      "Epoch 204/1000\n",
      "2335/2335 [==============================] - 0s 164us/step - loss: 23515.4430 - val_loss: 20038.9441\n",
      "Epoch 205/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 22738.7832 - val_loss: 19691.9285\n",
      "Epoch 206/1000\n",
      "2335/2335 [==============================] - 0s 138us/step - loss: 22558.2368 - val_loss: 19010.3520\n",
      "Epoch 207/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 22882.5486 - val_loss: 20537.7230\n",
      "Epoch 208/1000\n",
      "2335/2335 [==============================] - 0s 152us/step - loss: 22578.2062 - val_loss: 18482.1224\n",
      "Epoch 209/1000\n",
      "2335/2335 [==============================] - 0s 140us/step - loss: 22062.9092 - val_loss: 18539.4146\n",
      "Epoch 210/1000\n",
      "2335/2335 [==============================] - 0s 159us/step - loss: 22295.9180 - val_loss: 20599.2184\n",
      "Epoch 211/1000\n",
      "2335/2335 [==============================] - 0s 151us/step - loss: 22579.4987 - val_loss: 24749.0124\n",
      "Epoch 212/1000\n",
      "2335/2335 [==============================] - 0s 138us/step - loss: 22290.9634 - val_loss: 18204.1566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 213/1000\n",
      "2335/2335 [==============================] - 0s 148us/step - loss: 22313.1689 - val_loss: 19735.9576\n",
      "Epoch 214/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 22380.1793 - val_loss: 19507.8731\n",
      "Epoch 215/1000\n",
      "2335/2335 [==============================] - 0s 145us/step - loss: 22247.1349 - val_loss: 18793.0564\n",
      "Epoch 216/1000\n",
      "2335/2335 [==============================] - 0s 139us/step - loss: 22375.1663 - val_loss: 17822.6406\n",
      "Epoch 217/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 21889.1012 - val_loss: 19684.5136\n",
      "Epoch 218/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 22214.1601 - val_loss: 19831.2086\n",
      "Epoch 219/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 22172.2562 - val_loss: 17471.3204\n",
      "Epoch 220/1000\n",
      "2335/2335 [==============================] - 0s 164us/step - loss: 21905.5078 - val_loss: 17808.4695\n",
      "Epoch 221/1000\n",
      "2335/2335 [==============================] - 0s 136us/step - loss: 21916.3859 - val_loss: 17696.7988\n",
      "Epoch 222/1000\n",
      "2335/2335 [==============================] - 0s 136us/step - loss: 22063.1064 - val_loss: 18671.4541\n",
      "Epoch 223/1000\n",
      "2335/2335 [==============================] - 0s 156us/step - loss: 22543.9645 - val_loss: 24163.0678\n",
      "Epoch 224/1000\n",
      "2335/2335 [==============================] - 0s 144us/step - loss: 21789.7146 - val_loss: 18775.9890\n",
      "Epoch 225/1000\n",
      "2335/2335 [==============================] - 0s 148us/step - loss: 21882.6521 - val_loss: 19657.2277\n",
      "Epoch 226/1000\n",
      "2335/2335 [==============================] - 0s 144us/step - loss: 22183.0676 - val_loss: 18131.4830\n",
      "Epoch 227/1000\n",
      "2335/2335 [==============================] - 0s 153us/step - loss: 21613.7788 - val_loss: 18424.2096\n",
      "Epoch 228/1000\n",
      "2335/2335 [==============================] - 0s 152us/step - loss: 21710.8589 - val_loss: 17847.9174\n",
      "Epoch 229/1000\n",
      "2335/2335 [==============================] - 0s 136us/step - loss: 21898.2348 - val_loss: 17489.3177\n",
      "Epoch 230/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 21657.9562 - val_loss: 18566.4567\n",
      "Epoch 231/1000\n",
      "2335/2335 [==============================] - 0s 151us/step - loss: 21820.6687 - val_loss: 17578.8677\n",
      "Epoch 232/1000\n",
      "2335/2335 [==============================] - 0s 140us/step - loss: 21499.6655 - val_loss: 17698.1523\n",
      "Epoch 233/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 21956.4171 - val_loss: 17635.4514\n",
      "Epoch 234/1000\n",
      "2335/2335 [==============================] - 0s 156us/step - loss: 21543.2994 - val_loss: 17982.1167\n",
      "Epoch 235/1000\n",
      "2335/2335 [==============================] - 0s 136us/step - loss: 22436.9287 - val_loss: 16888.5310\n",
      "Epoch 236/1000\n",
      "2335/2335 [==============================] - 0s 171us/step - loss: 22101.6430 - val_loss: 16893.8716\n",
      "Epoch 237/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 21901.9704 - val_loss: 16997.2396\n",
      "Epoch 238/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 22108.7734 - val_loss: 18283.7135\n",
      "Epoch 239/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 20911.3332 - val_loss: 17261.5029\n",
      "Epoch 240/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 21803.4557 - val_loss: 18659.5486\n",
      "Epoch 241/1000\n",
      "2335/2335 [==============================] - 0s 156us/step - loss: 21236.9875 - val_loss: 17117.4539\n",
      "Epoch 242/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 21874.5861 - val_loss: 18127.3233\n",
      "Epoch 243/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 21603.4475 - val_loss: 20901.2536\n",
      "Epoch 244/1000\n",
      "2335/2335 [==============================] - 0s 156us/step - loss: 21177.1256 - val_loss: 16773.5159\n",
      "Epoch 245/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 20878.3590 - val_loss: 17105.2622\n",
      "Epoch 246/1000\n",
      "2335/2335 [==============================] - 0s 138us/step - loss: 21295.4875 - val_loss: 19964.0543\n",
      "Epoch 247/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 21022.0317 - val_loss: 18962.6347\n",
      "Epoch 248/1000\n",
      "2335/2335 [==============================] - 0s 155us/step - loss: 21674.1679 - val_loss: 17844.3182\n",
      "Epoch 249/1000\n",
      "2335/2335 [==============================] - 0s 137us/step - loss: 20861.9184 - val_loss: 17970.8151\n",
      "Epoch 250/1000\n",
      "2335/2335 [==============================] - 0s 159us/step - loss: 21062.2792 - val_loss: 23555.7982\n",
      "Epoch 251/1000\n",
      "2335/2335 [==============================] - 0s 155us/step - loss: 20771.9510 - val_loss: 19528.6209\n",
      "Epoch 252/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 21034.4165 - val_loss: 16393.2717\n",
      "Epoch 253/1000\n",
      "2335/2335 [==============================] - 0s 150us/step - loss: 20928.0621 - val_loss: 16670.1398\n",
      "Epoch 254/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 20589.3119 - val_loss: 17801.1092\n",
      "Epoch 255/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 20756.8137 - val_loss: 16796.3836\n",
      "Epoch 256/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 20940.3972 - val_loss: 18037.5219\n",
      "Epoch 257/1000\n",
      "2335/2335 [==============================] - 0s 164us/step - loss: 21057.1420 - val_loss: 18105.3087\n",
      "Epoch 258/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 20719.3550 - val_loss: 16380.9265\n",
      "Epoch 259/1000\n",
      "2335/2335 [==============================] - 0s 150us/step - loss: 20630.1189 - val_loss: 17651.1898\n",
      "Epoch 260/1000\n",
      "2335/2335 [==============================] - 0s 156us/step - loss: 20968.1618 - val_loss: 16019.1018\n",
      "Epoch 261/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 20830.9399 - val_loss: 18842.2593\n",
      "Epoch 262/1000\n",
      "2335/2335 [==============================] - 0s 140us/step - loss: 20481.1115 - val_loss: 16885.9330\n",
      "Epoch 263/1000\n",
      "2335/2335 [==============================] - 0s 164us/step - loss: 20501.2605 - val_loss: 16071.3396\n",
      "Epoch 264/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 20384.5423 - val_loss: 16078.5681\n",
      "Epoch 265/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 20673.8444 - val_loss: 16696.3984\n",
      "Epoch 266/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 20052.9321 - val_loss: 15806.7181\n",
      "Epoch 267/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 20804.5523 - val_loss: 16109.2041\n",
      "Epoch 268/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 20231.2946 - val_loss: 16530.9307\n",
      "Epoch 269/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 20493.3979 - val_loss: 19097.2947\n",
      "Epoch 270/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 20236.8531 - val_loss: 16243.3781\n",
      "Epoch 271/1000\n",
      "2335/2335 [==============================] - 0s 155us/step - loss: 20511.3789 - val_loss: 16724.1380\n",
      "Epoch 272/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 20370.5473 - val_loss: 21204.0292\n",
      "Epoch 273/1000\n",
      "2335/2335 [==============================] - 0s 168us/step - loss: 20885.7096 - val_loss: 16637.9103\n",
      "Epoch 274/1000\n",
      "2335/2335 [==============================] - 0s 150us/step - loss: 20048.2330 - val_loss: 15861.3721\n",
      "Epoch 275/1000\n",
      "2335/2335 [==============================] - 0s 145us/step - loss: 20208.9106 - val_loss: 19586.4442\n",
      "Epoch 276/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 20406.9004 - val_loss: 16480.2804\n",
      "Epoch 277/1000\n",
      "2335/2335 [==============================] - 0s 136us/step - loss: 20155.3156 - val_loss: 15524.7648\n",
      "Epoch 278/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 20633.2864 - val_loss: 21224.0852\n",
      "Epoch 279/1000\n",
      "2335/2335 [==============================] - 0s 151us/step - loss: 20461.0212 - val_loss: 19346.3901\n",
      "Epoch 280/1000\n",
      "2335/2335 [==============================] - 0s 147us/step - loss: 19976.6376 - val_loss: 16111.7349\n",
      "Epoch 281/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 20559.4026 - val_loss: 17249.7885\n",
      "Epoch 282/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 20299.9960 - val_loss: 16208.9540\n",
      "Epoch 283/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2335/2335 [==============================] - 0s 149us/step - loss: 20252.5685 - val_loss: 15662.7035\n",
      "Epoch 284/1000\n",
      "2335/2335 [==============================] - 0s 169us/step - loss: 20175.3164 - val_loss: 20452.9802\n",
      "Epoch 285/1000\n",
      "2335/2335 [==============================] - 0s 138us/step - loss: 20212.5322 - val_loss: 16564.2201\n",
      "Epoch 286/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 20007.6201 - val_loss: 15926.1189\n",
      "Epoch 287/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 20286.6164 - val_loss: 15717.9294\n",
      "Epoch 288/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 19854.9820 - val_loss: 16256.6871\n",
      "Epoch 289/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 20059.7847 - val_loss: 15284.8537\n",
      "Epoch 290/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 19347.5812 - val_loss: 15594.1394\n",
      "Epoch 291/1000\n",
      "2335/2335 [==============================] - 0s 141us/step - loss: 19873.5576 - val_loss: 15901.0007\n",
      "Epoch 292/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 19996.8447 - val_loss: 16088.2085\n",
      "Epoch 293/1000\n",
      "2335/2335 [==============================] - 0s 151us/step - loss: 19369.4141 - val_loss: 16422.0248\n",
      "Epoch 294/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 20014.0823 - val_loss: 16180.4569\n",
      "Epoch 295/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 20010.3708 - val_loss: 19461.3909\n",
      "Epoch 296/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 19997.3529 - val_loss: 17940.8598\n",
      "Epoch 297/1000\n",
      "2335/2335 [==============================] - 0s 138us/step - loss: 19907.0216 - val_loss: 15509.1833\n",
      "Epoch 298/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 20096.7148 - val_loss: 15915.1550\n",
      "Epoch 299/1000\n",
      "2335/2335 [==============================] - 0s 175us/step - loss: 19592.9361 - val_loss: 18194.8780\n",
      "Epoch 300/1000\n",
      "2335/2335 [==============================] - 0s 139us/step - loss: 19966.7170 - val_loss: 17750.0795\n",
      "Epoch 301/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 19867.5679 - val_loss: 18279.4343\n",
      "Epoch 302/1000\n",
      "2335/2335 [==============================] - 0s 162us/step - loss: 19714.2754 - val_loss: 15318.6934\n",
      "Epoch 303/1000\n",
      "2335/2335 [==============================] - 0s 137us/step - loss: 20012.1154 - val_loss: 14479.9733\n",
      "Epoch 304/1000\n",
      "2335/2335 [==============================] - 0s 160us/step - loss: 19389.6870 - val_loss: 15146.0206\n",
      "Epoch 305/1000\n",
      "2335/2335 [==============================] - 0s 145us/step - loss: 19375.6768 - val_loss: 15669.1468\n",
      "Epoch 306/1000\n",
      "2335/2335 [==============================] - 0s 165us/step - loss: 19428.2971 - val_loss: 15975.2978\n",
      "Epoch 307/1000\n",
      "2335/2335 [==============================] - 0s 164us/step - loss: 19323.6900 - val_loss: 16479.9995\n",
      "Epoch 308/1000\n",
      "2335/2335 [==============================] - 0s 150us/step - loss: 20050.2485 - val_loss: 15335.4112\n",
      "Epoch 309/1000\n",
      "2335/2335 [==============================] - 0s 156us/step - loss: 19755.7082 - val_loss: 19010.8606\n",
      "Epoch 310/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 19198.0146 - val_loss: 15531.7323\n",
      "Epoch 311/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 19759.8667 - val_loss: 16447.6357\n",
      "Epoch 312/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 20058.4641 - val_loss: 16082.1647\n",
      "Epoch 313/1000\n",
      "2335/2335 [==============================] - 0s 171us/step - loss: 19534.3364 - val_loss: 14340.0401\n",
      "Epoch 314/1000\n",
      "2335/2335 [==============================] - 0s 151us/step - loss: 19244.0428 - val_loss: 15527.6695\n",
      "Epoch 315/1000\n",
      "2335/2335 [==============================] - 0s 141us/step - loss: 19481.4354 - val_loss: 14860.3681\n",
      "Epoch 316/1000\n",
      "2335/2335 [==============================] - 0s 179us/step - loss: 19228.5014 - val_loss: 15915.9523\n",
      "Epoch 317/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 19134.7646 - val_loss: 14934.4177\n",
      "Epoch 318/1000\n",
      "2335/2335 [==============================] - 0s 155us/step - loss: 19322.4297 - val_loss: 14638.4079\n",
      "Epoch 319/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 19242.4641 - val_loss: 17462.1824\n",
      "Epoch 320/1000\n",
      "2335/2335 [==============================] - 0s 146us/step - loss: 19414.0233 - val_loss: 16396.3883\n",
      "Epoch 321/1000\n",
      "2335/2335 [==============================] - 0s 179us/step - loss: 19383.3477 - val_loss: 14487.9120\n",
      "Epoch 322/1000\n",
      "2335/2335 [==============================] - 0s 152us/step - loss: 19432.6925 - val_loss: 14493.3187\n",
      "Epoch 323/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 19159.5654 - val_loss: 14193.7560\n",
      "Epoch 324/1000\n",
      "2335/2335 [==============================] - 0s 171us/step - loss: 19446.2661 - val_loss: 14399.2298\n",
      "Epoch 325/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 19571.0214 - val_loss: 15307.0684\n",
      "Epoch 326/1000\n",
      "2335/2335 [==============================] - 0s 138us/step - loss: 19218.3099 - val_loss: 18734.4603\n",
      "Epoch 327/1000\n",
      "2335/2335 [==============================] - 0s 164us/step - loss: 19005.7596 - val_loss: 14931.8456\n",
      "Epoch 328/1000\n",
      "2335/2335 [==============================] - 0s 164us/step - loss: 19482.1352 - val_loss: 15478.8224\n",
      "Epoch 329/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 18868.4228 - val_loss: 14011.8446\n",
      "Epoch 330/1000\n",
      "2335/2335 [==============================] - 0s 178us/step - loss: 19046.7507 - val_loss: 16202.9606\n",
      "Epoch 331/1000\n",
      "2335/2335 [==============================] - 0s 151us/step - loss: 19422.7602 - val_loss: 15975.6785\n",
      "Epoch 332/1000\n",
      "2335/2335 [==============================] - 0s 155us/step - loss: 19793.0449 - val_loss: 14562.6327\n",
      "Epoch 333/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 18540.3821 - val_loss: 14200.6948\n",
      "Epoch 334/1000\n",
      "2335/2335 [==============================] - 0s 136us/step - loss: 19171.5163 - val_loss: 15770.8699\n",
      "Epoch 335/1000\n",
      "2335/2335 [==============================] - 0s 186us/step - loss: 19337.6767 - val_loss: 17189.7439\n",
      "Epoch 336/1000\n",
      "2335/2335 [==============================] - 0s 151us/step - loss: 18734.8152 - val_loss: 15136.9072\n",
      "Epoch 337/1000\n",
      "2335/2335 [==============================] - 0s 153us/step - loss: 19317.6388 - val_loss: 14640.1058\n",
      "Epoch 338/1000\n",
      "2335/2335 [==============================] - 0s 171us/step - loss: 18613.8700 - val_loss: 18808.7003\n",
      "Epoch 339/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 18892.3961 - val_loss: 14004.3664\n",
      "Epoch 340/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 19459.2130 - val_loss: 18561.8989\n",
      "Epoch 341/1000\n",
      "2335/2335 [==============================] - 0s 170us/step - loss: 19468.2224 - val_loss: 16874.7661\n",
      "Epoch 342/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 18646.5325 - val_loss: 14021.6992\n",
      "Epoch 343/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 18518.1491 - val_loss: 14674.1985\n",
      "Epoch 344/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 18920.1956 - val_loss: 14196.8853\n",
      "Epoch 345/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 18374.2776 - val_loss: 15298.0759\n",
      "Epoch 346/1000\n",
      "2335/2335 [==============================] - 0s 144us/step - loss: 18877.5422 - val_loss: 16808.0820\n",
      "Epoch 347/1000\n",
      "2335/2335 [==============================] - 0s 162us/step - loss: 19125.5289 - val_loss: 16705.3047\n",
      "Epoch 348/1000\n",
      "2335/2335 [==============================] - 0s 138us/step - loss: 18864.0720 - val_loss: 16762.6701\n",
      "Epoch 349/1000\n",
      "2335/2335 [==============================] - 0s 155us/step - loss: 19027.1407 - val_loss: 17951.5024\n",
      "Epoch 350/1000\n",
      "2335/2335 [==============================] - 0s 159us/step - loss: 18208.2624 - val_loss: 14340.6924\n",
      "Epoch 351/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 18674.5103 - val_loss: 13825.4630\n",
      "Epoch 352/1000\n",
      "2335/2335 [==============================] - 0s 159us/step - loss: 18495.3539 - val_loss: 15068.4781\n",
      "Epoch 353/1000\n",
      "2335/2335 [==============================] - 0s 156us/step - loss: 18419.7644 - val_loss: 15558.2859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 354/1000\n",
      "2335/2335 [==============================] - 0s 136us/step - loss: 18685.3581 - val_loss: 14033.9517\n",
      "Epoch 355/1000\n",
      "2335/2335 [==============================] - 0s 161us/step - loss: 18557.3335 - val_loss: 14036.2630\n",
      "Epoch 356/1000\n",
      "2335/2335 [==============================] - 0s 151us/step - loss: 17939.9023 - val_loss: 14386.9258\n",
      "Epoch 357/1000\n",
      "2335/2335 [==============================] - 0s 165us/step - loss: 18576.7141 - val_loss: 13861.0862\n",
      "Epoch 358/1000\n",
      "2335/2335 [==============================] - 0s 171us/step - loss: 18503.3164 - val_loss: 16163.3122\n",
      "Epoch 359/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 18800.1746 - val_loss: 14783.9769\n",
      "Epoch 360/1000\n",
      "2335/2335 [==============================] - 0s 152us/step - loss: 18610.8581 - val_loss: 19619.8199\n",
      "Epoch 361/1000\n",
      "2335/2335 [==============================] - 0s 155us/step - loss: 18228.0142 - val_loss: 22645.5360\n",
      "Epoch 362/1000\n",
      "2335/2335 [==============================] - 0s 136us/step - loss: 18563.5157 - val_loss: 14446.1548\n",
      "Epoch 363/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 18034.1838 - val_loss: 14194.4123\n",
      "Epoch 364/1000\n",
      "2335/2335 [==============================] - 0s 184us/step - loss: 18254.9758 - val_loss: 14344.5223\n",
      "Epoch 365/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 18529.5166 - val_loss: 15552.7788\n",
      "Epoch 366/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 18450.8795 - val_loss: 13965.0884\n",
      "Epoch 367/1000\n",
      "2335/2335 [==============================] - 0s 164us/step - loss: 18102.8862 - val_loss: 14129.1639\n",
      "Epoch 368/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 18075.1992 - val_loss: 15081.1380\n",
      "Epoch 369/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 18226.0844 - val_loss: 17416.4993\n",
      "Epoch 370/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 18231.1114 - val_loss: 22508.7165\n",
      "Epoch 371/1000\n",
      "2335/2335 [==============================] - 0s 174us/step - loss: 18472.6413 - val_loss: 13904.2834\n",
      "Epoch 372/1000\n",
      "2335/2335 [==============================] - 0s 139us/step - loss: 18649.0701 - val_loss: 13444.4333\n",
      "Epoch 373/1000\n",
      "2335/2335 [==============================] - 0s 164us/step - loss: 18697.4489 - val_loss: 13790.6119\n",
      "Epoch 374/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 18273.9074 - val_loss: 12988.3689\n",
      "Epoch 375/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 18027.9358 - val_loss: 14409.1455\n",
      "Epoch 376/1000\n",
      "2335/2335 [==============================] - 0s 171us/step - loss: 18900.2022 - val_loss: 13251.7398\n",
      "Epoch 377/1000\n",
      "2335/2335 [==============================] - 0s 165us/step - loss: 17806.7376 - val_loss: 13135.9749\n",
      "Epoch 378/1000\n",
      "2335/2335 [==============================] - 0s 165us/step - loss: 18155.7437 - val_loss: 13223.2331\n",
      "Epoch 379/1000\n",
      "2335/2335 [==============================] - 0s 177us/step - loss: 17881.0637 - val_loss: 15562.8985\n",
      "Epoch 380/1000\n",
      "2335/2335 [==============================] - 0s 153us/step - loss: 18314.9118 - val_loss: 14448.5047\n",
      "Epoch 381/1000\n",
      "2335/2335 [==============================] - 0s 161us/step - loss: 18372.5770 - val_loss: 14437.5930\n",
      "Epoch 382/1000\n",
      "2335/2335 [==============================] - 0s 171us/step - loss: 18438.7915 - val_loss: 13464.8092\n",
      "Epoch 383/1000\n",
      "2335/2335 [==============================] - 0s 164us/step - loss: 17720.3800 - val_loss: 13073.1665\n",
      "Epoch 384/1000\n",
      "2335/2335 [==============================] - 0s 205us/step - loss: 18444.2820 - val_loss: 26371.7091\n",
      "Epoch 385/1000\n",
      "2335/2335 [==============================] - 0s 180us/step - loss: 18666.0960 - val_loss: 12820.0684\n",
      "Epoch 386/1000\n",
      "2335/2335 [==============================] - 0s 200us/step - loss: 18029.2960 - val_loss: 14750.0032\n",
      "Epoch 387/1000\n",
      "2335/2335 [==============================] - 0s 177us/step - loss: 18136.8830 - val_loss: 13244.0798\n",
      "Epoch 388/1000\n",
      "2335/2335 [==============================] - 0s 165us/step - loss: 18089.4860 - val_loss: 14058.4385\n",
      "Epoch 389/1000\n",
      "2335/2335 [==============================] - 0s 199us/step - loss: 18255.5369 - val_loss: 25080.3026\n",
      "Epoch 390/1000\n",
      "2335/2335 [==============================] - 0s 193us/step - loss: 17730.0720 - val_loss: 13676.6578\n",
      "Epoch 391/1000\n",
      "2335/2335 [==============================] - 0s 165us/step - loss: 17887.2330 - val_loss: 15602.4171\n",
      "Epoch 392/1000\n",
      "2335/2335 [==============================] - 0s 171us/step - loss: 17996.2109 - val_loss: 15227.3076\n",
      "Epoch 393/1000\n",
      "2335/2335 [==============================] - 0s 165us/step - loss: 18029.7686 - val_loss: 13107.8628\n",
      "Epoch 394/1000\n",
      "2335/2335 [==============================] - 0s 159us/step - loss: 17773.1159 - val_loss: 16230.2169\n",
      "Epoch 395/1000\n",
      "2335/2335 [==============================] - 0s 177us/step - loss: 18036.5449 - val_loss: 13129.3603\n",
      "Epoch 396/1000\n",
      "2335/2335 [==============================] - 0s 156us/step - loss: 18128.2205 - val_loss: 19497.9250\n",
      "Epoch 397/1000\n",
      "2335/2335 [==============================] - 0s 175us/step - loss: 17905.6158 - val_loss: 13130.7565\n",
      "Epoch 398/1000\n",
      "2335/2335 [==============================] - 0s 181us/step - loss: 18304.9915 - val_loss: 14747.1475\n",
      "Epoch 399/1000\n",
      "2335/2335 [==============================] - 0s 164us/step - loss: 17853.7727 - val_loss: 12850.4490\n",
      "Epoch 400/1000\n",
      "2335/2335 [==============================] - 0s 186us/step - loss: 17815.6002 - val_loss: 13637.4574\n",
      "Epoch 401/1000\n",
      "2335/2335 [==============================] - 0s 192us/step - loss: 17811.3479 - val_loss: 12905.6036\n",
      "Epoch 402/1000\n",
      "2335/2335 [==============================] - 0s 186us/step - loss: 17577.9877 - val_loss: 12601.0051\n",
      "Epoch 403/1000\n",
      "2335/2335 [==============================] - 0s 170us/step - loss: 18157.8363 - val_loss: 13772.8146\n",
      "Epoch 404/1000\n",
      "2335/2335 [==============================] - 0s 180us/step - loss: 18255.6897 - val_loss: 13825.1263\n",
      "Epoch 405/1000\n",
      "2335/2335 [==============================] - 0s 172us/step - loss: 17767.7341 - val_loss: 12734.7718\n",
      "Epoch 406/1000\n",
      "2335/2335 [==============================] - 0s 165us/step - loss: 17254.8009 - val_loss: 16114.4426\n",
      "Epoch 407/1000\n",
      "2335/2335 [==============================] - 0s 190us/step - loss: 17747.7900 - val_loss: 14454.7409\n",
      "Epoch 408/1000\n",
      "2335/2335 [==============================] - 0s 169us/step - loss: 18085.9630 - val_loss: 13049.9435\n",
      "Epoch 409/1000\n",
      "2335/2335 [==============================] - 0s 188us/step - loss: 18298.0898 - val_loss: 17199.9918\n",
      "Epoch 410/1000\n",
      "2335/2335 [==============================] - 0s 204us/step - loss: 17677.2990 - val_loss: 12655.7534\n",
      "Epoch 411/1000\n",
      "2335/2335 [==============================] - 0s 179us/step - loss: 18003.5757 - val_loss: 13521.4968\n",
      "Epoch 412/1000\n",
      "2335/2335 [==============================] - 0s 164us/step - loss: 17693.1376 - val_loss: 13179.7334\n",
      "Epoch 413/1000\n",
      "2335/2335 [==============================] - 0s 199us/step - loss: 17720.9955 - val_loss: 13085.3232\n",
      "Epoch 414/1000\n",
      "2335/2335 [==============================] - 0s 175us/step - loss: 17392.8493 - val_loss: 12447.2698\n",
      "Epoch 415/1000\n",
      "2335/2335 [==============================] - 0s 182us/step - loss: 17645.3804 - val_loss: 12831.3762\n",
      "Epoch 416/1000\n",
      "2335/2335 [==============================] - 0s 177us/step - loss: 17725.8732 - val_loss: 17165.4694\n",
      "Epoch 417/1000\n",
      "2335/2335 [==============================] - 0s 184us/step - loss: 16897.2173 - val_loss: 12888.4059\n",
      "Epoch 418/1000\n",
      "2335/2335 [==============================] - 0s 189us/step - loss: 17835.5216 - val_loss: 13153.0030\n",
      "Epoch 419/1000\n",
      "2335/2335 [==============================] - 0s 169us/step - loss: 18106.4118 - val_loss: 15323.6132\n",
      "Epoch 420/1000\n",
      "2335/2335 [==============================] - 0s 191us/step - loss: 18069.6472 - val_loss: 15587.7110\n",
      "Epoch 421/1000\n",
      "2335/2335 [==============================] - 0s 165us/step - loss: 18643.5299 - val_loss: 17526.2336\n",
      "Epoch 422/1000\n",
      "2335/2335 [==============================] - 0s 178us/step - loss: 17511.2064 - val_loss: 13366.4243\n",
      "Epoch 423/1000\n",
      "2335/2335 [==============================] - 0s 207us/step - loss: 17125.9430 - val_loss: 12537.9995\n",
      "Epoch 424/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2335/2335 [==============================] - 0s 200us/step - loss: 17590.4431 - val_loss: 17201.1312\n",
      "Epoch 425/1000\n",
      "2335/2335 [==============================] - 0s 164us/step - loss: 17975.3686 - val_loss: 15214.3591\n",
      "Epoch 426/1000\n",
      "2335/2335 [==============================] - 0s 152us/step - loss: 17083.5503 - val_loss: 15870.2368\n",
      "Epoch 427/1000\n",
      "2335/2335 [==============================] - 0s 163us/step - loss: 17395.7336 - val_loss: 16432.4720\n",
      "Epoch 428/1000\n",
      "2335/2335 [==============================] - 0s 146us/step - loss: 18294.3345 - val_loss: 12454.6424\n",
      "Epoch 429/1000\n",
      "2335/2335 [==============================] - 0s 152us/step - loss: 17557.1510 - val_loss: 12531.8523\n",
      "Epoch 430/1000\n",
      "2335/2335 [==============================] - 0s 156us/step - loss: 18137.7184 - val_loss: 15478.1089\n",
      "Epoch 431/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 17332.1179 - val_loss: 13277.1341\n",
      "Epoch 432/1000\n",
      "2335/2335 [==============================] - 0s 156us/step - loss: 17227.2579 - val_loss: 20280.9653\n",
      "Epoch 433/1000\n",
      "2335/2335 [==============================] - 0s 150us/step - loss: 17692.3480 - val_loss: 12875.7510\n",
      "Epoch 434/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 18034.1973 - val_loss: 13112.3820\n",
      "Epoch 435/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 17755.5407 - val_loss: 12200.3043\n",
      "Epoch 436/1000\n",
      "2335/2335 [==============================] - 0s 163us/step - loss: 17141.7248 - val_loss: 12214.9204\n",
      "Epoch 437/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 17784.0957 - val_loss: 14583.8955\n",
      "Epoch 438/1000\n",
      "2335/2335 [==============================] - 0s 155us/step - loss: 17685.8987 - val_loss: 14006.9527\n",
      "Epoch 439/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 17647.5337 - val_loss: 13827.3273\n",
      "Epoch 440/1000\n",
      "2335/2335 [==============================] - 0s 140us/step - loss: 18237.6594 - val_loss: 14644.6699\n",
      "Epoch 441/1000\n",
      "2335/2335 [==============================] - 0s 156us/step - loss: 17418.4590 - val_loss: 12573.8728\n",
      "Epoch 442/1000\n",
      "2335/2335 [==============================] - 0s 150us/step - loss: 17279.7970 - val_loss: 12485.8605\n",
      "Epoch 443/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 18220.1065 - val_loss: 12221.0239\n",
      "Epoch 444/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 17897.0498 - val_loss: 13635.3939\n",
      "Epoch 445/1000\n",
      "2335/2335 [==============================] - 0s 153us/step - loss: 16915.2569 - val_loss: 12772.1336\n",
      "Epoch 446/1000\n",
      "2335/2335 [==============================] - 0s 151us/step - loss: 17026.1365 - val_loss: 13241.3175\n",
      "Epoch 447/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 18068.3154 - val_loss: 12608.7549\n",
      "Epoch 448/1000\n",
      "2335/2335 [==============================] - 0s 141us/step - loss: 17284.0190 - val_loss: 12446.8731\n",
      "Epoch 449/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 17537.2051 - val_loss: 12354.5880\n",
      "Epoch 450/1000\n",
      "2335/2335 [==============================] - 0s 147us/step - loss: 17676.5665 - val_loss: 12386.1195\n",
      "Epoch 451/1000\n",
      "2335/2335 [==============================] - ETA: 0s - loss: 17779.513 - 0s 164us/step - loss: 17781.4976 - val_loss: 13663.0866\n",
      "Epoch 452/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 17172.2428 - val_loss: 11988.4583\n",
      "Epoch 453/1000\n",
      "2335/2335 [==============================] - 0s 171us/step - loss: 17070.7418 - val_loss: 12579.6809\n",
      "Epoch 454/1000\n",
      "2335/2335 [==============================] - 0s 164us/step - loss: 16845.5920 - val_loss: 12217.8214\n",
      "Epoch 455/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 17179.0709 - val_loss: 12520.7982\n",
      "Epoch 456/1000\n",
      "2335/2335 [==============================] - 0s 169us/step - loss: 17312.2850 - val_loss: 16518.6425\n",
      "Epoch 457/1000\n",
      "2335/2335 [==============================] - 0s 155us/step - loss: 17539.6805 - val_loss: 12327.9767\n",
      "Epoch 458/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 17305.8018 - val_loss: 13979.0285\n",
      "Epoch 459/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 17453.9004 - val_loss: 13897.0650\n",
      "Epoch 460/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 17077.9281 - val_loss: 12361.6724\n",
      "Epoch 461/1000\n",
      "2335/2335 [==============================] - 0s 150us/step - loss: 17149.2026 - val_loss: 15687.6833\n",
      "Epoch 462/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 17336.4615 - val_loss: 15351.3872\n",
      "Epoch 463/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 17491.4843 - val_loss: 12390.9748\n",
      "Epoch 464/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 16781.7909 - val_loss: 12357.2345\n",
      "Epoch 465/1000\n",
      "2335/2335 [==============================] - 0s 150us/step - loss: 16824.9930 - val_loss: 13483.1597\n",
      "Epoch 466/1000\n",
      "2335/2335 [==============================] - 0s 156us/step - loss: 17187.7428 - val_loss: 14204.1051\n",
      "Epoch 467/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 16937.1490 - val_loss: 12154.2032\n",
      "Epoch 468/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 17635.8239 - val_loss: 11939.7968\n",
      "Epoch 469/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 17610.6006 - val_loss: 12283.6452\n",
      "Epoch 470/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 16773.0943 - val_loss: 13327.6407\n",
      "Epoch 471/1000\n",
      "2335/2335 [==============================] - 0s 150us/step - loss: 16919.2933 - val_loss: 11787.2020\n",
      "Epoch 472/1000\n",
      "2335/2335 [==============================] - 0s 164us/step - loss: 16756.4750 - val_loss: 12594.1028\n",
      "Epoch 473/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 16744.1522 - val_loss: 13197.7485\n",
      "Epoch 474/1000\n",
      "2335/2335 [==============================] - 0s 141us/step - loss: 17311.1515 - val_loss: 12090.4800\n",
      "Epoch 475/1000\n",
      "2335/2335 [==============================] - 0s 162us/step - loss: 16979.2199 - val_loss: 14113.4422\n",
      "Epoch 476/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 17148.6712 - val_loss: 12045.9781\n",
      "Epoch 477/1000\n",
      "2335/2335 [==============================] - 0s 140us/step - loss: 17173.8260 - val_loss: 13067.3602\n",
      "Epoch 478/1000\n",
      "2335/2335 [==============================] - 0s 185us/step - loss: 17456.4095 - val_loss: 12810.4592\n",
      "Epoch 479/1000\n",
      "2335/2335 [==============================] - 0s 178us/step - loss: 17374.1729 - val_loss: 14434.0512\n",
      "Epoch 480/1000\n",
      "2335/2335 [==============================] - 0s 178us/step - loss: 16923.3239 - val_loss: 13387.4803\n",
      "Epoch 481/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 17059.6405 - val_loss: 13024.5105\n",
      "Epoch 482/1000\n",
      "2335/2335 [==============================] - 0s 164us/step - loss: 17051.2862 - val_loss: 13810.3611\n",
      "Epoch 483/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 17372.8486 - val_loss: 12169.1373\n",
      "Epoch 484/1000\n",
      "2335/2335 [==============================] - 0s 162us/step - loss: 16866.9917 - val_loss: 15096.7276\n",
      "Epoch 485/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 16783.0221 - val_loss: 14185.4558\n",
      "Epoch 486/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 16754.7605 - val_loss: 12118.3572\n",
      "Epoch 487/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 17633.3616 - val_loss: 18489.3732\n",
      "Epoch 488/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 16777.3586 - val_loss: 17985.0786\n",
      "Epoch 489/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 16887.5208 - val_loss: 12353.0317\n",
      "Epoch 490/1000\n",
      "2335/2335 [==============================] - 0s 170us/step - loss: 16768.2681 - val_loss: 12917.2629\n",
      "Epoch 491/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 16646.3398 - val_loss: 13510.0339\n",
      "Epoch 492/1000\n",
      "2335/2335 [==============================] - 0s 152us/step - loss: 16641.0819 - val_loss: 14047.9151\n",
      "Epoch 493/1000\n",
      "2335/2335 [==============================] - 0s 155us/step - loss: 17886.0438 - val_loss: 11687.6555\n",
      "Epoch 494/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2335/2335 [==============================] - 0s 142us/step - loss: 16613.6117 - val_loss: 13296.3158\n",
      "Epoch 495/1000\n",
      "2335/2335 [==============================] - 0s 155us/step - loss: 17060.4723 - val_loss: 12484.7360\n",
      "Epoch 496/1000\n",
      "2335/2335 [==============================] - 0s 153us/step - loss: 16993.4390 - val_loss: 18279.3268\n",
      "Epoch 497/1000\n",
      "2335/2335 [==============================] - 0s 140us/step - loss: 17023.2054 - val_loss: 11937.2083\n",
      "Epoch 498/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 16929.6567 - val_loss: 12659.9845\n",
      "Epoch 499/1000\n",
      "2335/2335 [==============================] - 0s 153us/step - loss: 16617.3942 - val_loss: 16467.1810\n",
      "Epoch 500/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 16827.5919 - val_loss: 16108.2486\n",
      "Epoch 501/1000\n",
      "2335/2335 [==============================] - 0s 160us/step - loss: 17667.7796 - val_loss: 12445.2777\n",
      "Epoch 502/1000\n",
      "2335/2335 [==============================] - 0s 153us/step - loss: 16552.6493 - val_loss: 15112.5569\n",
      "Epoch 503/1000\n",
      "2335/2335 [==============================] - 0s 148us/step - loss: 16791.4184 - val_loss: 12796.8217\n",
      "Epoch 504/1000\n",
      "2335/2335 [==============================] - 0s 152us/step - loss: 16809.8593 - val_loss: 13773.0635\n",
      "Epoch 505/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 16846.7073 - val_loss: 12653.1708\n",
      "Epoch 506/1000\n",
      "2335/2335 [==============================] - 0s 135us/step - loss: 16836.1880 - val_loss: 16075.9220\n",
      "Epoch 507/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 16589.7644 - val_loss: 13360.7608\n",
      "Epoch 508/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 16401.5754 - val_loss: 13749.3380\n",
      "Epoch 509/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 16559.7268 - val_loss: 12391.3514\n",
      "Epoch 510/1000\n",
      "2335/2335 [==============================] - 0s 146us/step - loss: 17545.2771 - val_loss: 12828.4992\n",
      "Epoch 511/1000\n",
      "2335/2335 [==============================] - 0s 153us/step - loss: 16798.9431 - val_loss: 13210.1741\n",
      "Epoch 512/1000\n",
      "2335/2335 [==============================] - 0s 150us/step - loss: 16546.3114 - val_loss: 13294.9116\n",
      "Epoch 513/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 16450.6919 - val_loss: 12917.0021\n",
      "Epoch 514/1000\n",
      "2335/2335 [==============================] - 0s 153us/step - loss: 16733.7370 - val_loss: 11962.7017\n",
      "Epoch 515/1000\n",
      "2335/2335 [==============================] - 0s 138us/step - loss: 17039.1383 - val_loss: 12216.6753\n",
      "Epoch 516/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 16636.3418 - val_loss: 15108.0882\n",
      "Epoch 517/1000\n",
      "2335/2335 [==============================] - 0s 151us/step - loss: 17447.8390 - val_loss: 16278.0837\n",
      "Epoch 518/1000\n",
      "2335/2335 [==============================] - 0s 134us/step - loss: 16467.9265 - val_loss: 12325.7556\n",
      "Epoch 519/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 16825.9142 - val_loss: 13022.3059\n",
      "Epoch 520/1000\n",
      "2335/2335 [==============================] - 0s 141us/step - loss: 16951.1503 - val_loss: 12492.4892\n",
      "Epoch 521/1000\n",
      "2335/2335 [==============================] - 0s 136us/step - loss: 16634.0487 - val_loss: 13149.9905\n",
      "Epoch 522/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 16356.3148 - val_loss: 13058.7876\n",
      "Epoch 523/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 16632.4420 - val_loss: 12637.0164\n",
      "Epoch 524/1000\n",
      "2335/2335 [==============================] - 0s 136us/step - loss: 16724.8510 - val_loss: 11968.0502\n",
      "Epoch 525/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 17035.2436 - val_loss: 13886.7074\n",
      "Epoch 526/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 16626.8238 - val_loss: 13656.7900\n",
      "Epoch 527/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 16524.1759 - val_loss: 12412.8549\n",
      "Epoch 528/1000\n",
      "2335/2335 [==============================] - 0s 171us/step - loss: 16939.5611 - val_loss: 12738.9376\n",
      "Epoch 529/1000\n",
      "2335/2335 [==============================] - 0s 148us/step - loss: 17505.9538 - val_loss: 12876.9465\n",
      "Epoch 530/1000\n",
      "2335/2335 [==============================] - 0s 144us/step - loss: 16896.5163 - val_loss: 15542.7541\n",
      "Epoch 531/1000\n",
      "2335/2335 [==============================] - 0s 159us/step - loss: 16980.2828 - val_loss: 14077.6604\n",
      "Epoch 532/1000\n",
      "2335/2335 [==============================] - 0s 151us/step - loss: 16583.8043 - val_loss: 12522.9779\n",
      "Epoch 533/1000\n",
      "2335/2335 [==============================] - 0s 138us/step - loss: 15796.1784 - val_loss: 12869.7365\n",
      "Epoch 534/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 16490.9949 - val_loss: 13074.1729\n",
      "Epoch 535/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 16789.0205 - val_loss: 11820.5218\n",
      "Epoch 536/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 17122.4031 - val_loss: 13538.5681\n",
      "Epoch 537/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 16613.9705 - val_loss: 12929.3649\n",
      "Epoch 538/1000\n",
      "2335/2335 [==============================] - 0s 164us/step - loss: 16380.1013 - val_loss: 13477.3767\n",
      "Epoch 539/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 16533.1625 - val_loss: 13502.1649\n",
      "Epoch 540/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 16157.4734 - val_loss: 12276.6011\n",
      "Epoch 541/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 16536.2414 - val_loss: 12022.0322\n",
      "Epoch 542/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 16737.9748 - val_loss: 12297.3563\n",
      "Epoch 543/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 16382.4642 - val_loss: 11917.2699\n",
      "Epoch 544/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 16837.1201 - val_loss: 22438.5193\n",
      "Epoch 545/1000\n",
      "2335/2335 [==============================] - 0s 150us/step - loss: 16596.4957 - val_loss: 12847.9890\n",
      "Epoch 546/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 16475.7346 - val_loss: 16928.6182\n",
      "Epoch 547/1000\n",
      "2335/2335 [==============================] - 0s 152us/step - loss: 16007.0184 - val_loss: 11789.5679\n",
      "Epoch 548/1000\n",
      "2335/2335 [==============================] - 0s 140us/step - loss: 16676.4475 - val_loss: 12479.0657\n",
      "Epoch 549/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 16780.5263 - val_loss: 13680.5410\n",
      "Epoch 550/1000\n",
      "2335/2335 [==============================] - 0s 153us/step - loss: 17093.0789 - val_loss: 12866.1557\n",
      "Epoch 551/1000\n",
      "2335/2335 [==============================] - 0s 145us/step - loss: 16646.5595 - val_loss: 12482.3158\n",
      "Epoch 552/1000\n",
      "2335/2335 [==============================] - 0s 156us/step - loss: 16756.5899 - val_loss: 11724.3399\n",
      "Epoch 553/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 17007.7666 - val_loss: 12336.3243\n",
      "Epoch 554/1000\n",
      "2335/2335 [==============================] - 0s 150us/step - loss: 16441.0001 - val_loss: 12781.9663\n",
      "Epoch 555/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 17172.7618 - val_loss: 13002.2082\n",
      "Epoch 556/1000\n",
      "2335/2335 [==============================] - 0s 156us/step - loss: 16671.9434 - val_loss: 12572.9632\n",
      "Epoch 557/1000\n",
      "2335/2335 [==============================] - 0s 145us/step - loss: 16539.9869 - val_loss: 11589.1709\n",
      "Epoch 558/1000\n",
      "2335/2335 [==============================] - 0s 155us/step - loss: 16126.9950 - val_loss: 12253.3194\n",
      "Epoch 559/1000\n",
      "2335/2335 [==============================] - 0s 155us/step - loss: 16519.5969 - val_loss: 12059.1473\n",
      "Epoch 560/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 16247.2037 - val_loss: 11963.3327\n",
      "Epoch 561/1000\n",
      "2335/2335 [==============================] - 0s 137us/step - loss: 15994.5132 - val_loss: 12914.9017\n",
      "Epoch 562/1000\n",
      "2335/2335 [==============================] - 0s 160us/step - loss: 15568.9758 - val_loss: 11383.0790\n",
      "Epoch 563/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 16371.6047 - val_loss: 12132.6483\n",
      "Epoch 564/1000\n",
      "2335/2335 [==============================] - 0s 138us/step - loss: 16297.9425 - val_loss: 11921.5602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 565/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 16022.8705 - val_loss: 11895.4877\n",
      "Epoch 566/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 16136.2906 - val_loss: 16551.8694\n",
      "Epoch 567/1000\n",
      "2335/2335 [==============================] - 0s 138us/step - loss: 16680.4173 - val_loss: 12884.5004\n",
      "Epoch 568/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 16049.9597 - val_loss: 12092.2367\n",
      "Epoch 569/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 16389.7087 - val_loss: 14466.1648\n",
      "Epoch 570/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 16218.0992 - val_loss: 12294.7626\n",
      "Epoch 571/1000\n",
      "2335/2335 [==============================] - 0s 153us/step - loss: 16490.7197 - val_loss: 13173.3404\n",
      "Epoch 572/1000\n",
      "2335/2335 [==============================] - 0s 139us/step - loss: 16039.6909 - val_loss: 11668.1659\n",
      "Epoch 573/1000\n",
      "2335/2335 [==============================] - 0s 160us/step - loss: 16610.4953 - val_loss: 14520.2163\n",
      "Epoch 574/1000\n",
      "2335/2335 [==============================] - 0s 152us/step - loss: 16302.6915 - val_loss: 13053.8146\n",
      "Epoch 575/1000\n",
      "2335/2335 [==============================] - 0s 136us/step - loss: 16231.9327 - val_loss: 12327.2940\n",
      "Epoch 576/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 16261.8301 - val_loss: 13702.9590\n",
      "Epoch 577/1000\n",
      "2335/2335 [==============================] - 0s 150us/step - loss: 16514.2889 - val_loss: 12902.8355\n",
      "Epoch 578/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 16195.6800 - val_loss: 11763.5646\n",
      "Epoch 579/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 16117.6777 - val_loss: 14195.1227\n",
      "Epoch 580/1000\n",
      "2335/2335 [==============================] - 0s 152us/step - loss: 16907.7778 - val_loss: 12761.6003\n",
      "Epoch 581/1000\n",
      "2335/2335 [==============================] - 0s 140us/step - loss: 16811.2646 - val_loss: 11999.4234\n",
      "Epoch 582/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 16062.1582 - val_loss: 14671.2616\n",
      "Epoch 583/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 16673.0846 - val_loss: 14000.6072\n",
      "Epoch 584/1000\n",
      "2335/2335 [==============================] - 0s 144us/step - loss: 16541.5691 - val_loss: 11896.8203\n",
      "Epoch 585/1000\n",
      "2335/2335 [==============================] - 0s 153us/step - loss: 16306.9469 - val_loss: 12193.9501\n",
      "Epoch 586/1000\n",
      "2335/2335 [==============================] - 0s 138us/step - loss: 16602.3941 - val_loss: 11891.2432\n",
      "Epoch 587/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 15671.9281 - val_loss: 12044.5846\n",
      "Epoch 588/1000\n",
      "2335/2335 [==============================] - 0s 156us/step - loss: 16068.9512 - val_loss: 13120.1586\n",
      "Epoch 589/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 16065.7392 - val_loss: 12219.1701\n",
      "Epoch 590/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 16385.7667 - val_loss: 12174.5855\n",
      "Epoch 591/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 16077.7101 - val_loss: 12324.1046\n",
      "Epoch 592/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 16227.7400 - val_loss: 13681.4185\n",
      "Epoch 593/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 15646.0127 - val_loss: 11791.8581\n",
      "Epoch 594/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 16323.4394 - val_loss: 12113.5976\n",
      "Epoch 595/1000\n",
      "2335/2335 [==============================] - 0s 155us/step - loss: 16217.3531 - val_loss: 14725.6290\n",
      "Epoch 596/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 15813.1090 - val_loss: 13034.4005\n",
      "Epoch 597/1000\n",
      "2335/2335 [==============================] - 0s 151us/step - loss: 16326.9045 - val_loss: 13107.8929\n",
      "Epoch 598/1000\n",
      "2335/2335 [==============================] - 0s 152us/step - loss: 16523.2833 - val_loss: 11810.6098\n",
      "Epoch 599/1000\n",
      "2335/2335 [==============================] - 0s 153us/step - loss: 16505.6553 - val_loss: 12682.1243\n",
      "Epoch 600/1000\n",
      "2335/2335 [==============================] - 0s 150us/step - loss: 16255.1148 - val_loss: 12494.5157\n",
      "Epoch 601/1000\n",
      "2335/2335 [==============================] - 0s 140us/step - loss: 15800.4681 - val_loss: 13170.8377\n",
      "Epoch 602/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 15752.8821 - val_loss: 13928.6413\n",
      "Epoch 603/1000\n",
      "2335/2335 [==============================] - 0s 156us/step - loss: 16162.1973 - val_loss: 15687.2487\n",
      "Epoch 604/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 15998.1073 - val_loss: 19126.0945\n",
      "Epoch 605/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 16107.1440 - val_loss: 16377.3072\n",
      "Epoch 606/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 16166.0676 - val_loss: 15953.5962\n",
      "Epoch 607/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 16398.1939 - val_loss: 11782.2514\n",
      "Epoch 608/1000\n",
      "2335/2335 [==============================] - 0s 171us/step - loss: 15716.9281 - val_loss: 11908.1753\n",
      "Epoch 609/1000\n",
      "2335/2335 [==============================] - 0s 155us/step - loss: 16036.7521 - val_loss: 12920.2242\n",
      "Epoch 610/1000\n",
      "2335/2335 [==============================] - 0s 144us/step - loss: 15766.5035 - val_loss: 11977.2586\n",
      "Epoch 611/1000\n",
      "2335/2335 [==============================] - 0s 164us/step - loss: 16292.8384 - val_loss: 20478.6187\n",
      "Epoch 612/1000\n",
      "2335/2335 [==============================] - 0s 155us/step - loss: 16756.6918 - val_loss: 11951.2264\n",
      "Epoch 613/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 15871.2011 - val_loss: 12293.2443\n",
      "Epoch 614/1000\n",
      "2335/2335 [==============================] - 0s 166us/step - loss: 16331.8817 - val_loss: 13271.5688\n",
      "Epoch 615/1000\n",
      "2335/2335 [==============================] - 0s 155us/step - loss: 15704.4027 - val_loss: 13480.4527\n",
      "Epoch 616/1000\n",
      "2335/2335 [==============================] - 0s 144us/step - loss: 16172.5701 - val_loss: 14330.2831\n",
      "Epoch 617/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 15786.8724 - val_loss: 12501.5069\n",
      "Epoch 618/1000\n",
      "2335/2335 [==============================] - 0s 164us/step - loss: 15557.7061 - val_loss: 12160.2496\n",
      "Epoch 619/1000\n",
      "2335/2335 [==============================] - 0s 150us/step - loss: 16043.3674 - val_loss: 12409.0965\n",
      "Epoch 620/1000\n",
      "2335/2335 [==============================] - 0s 160us/step - loss: 15765.3176 - val_loss: 12085.2292\n",
      "Epoch 621/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 15833.8205 - val_loss: 12105.1034\n",
      "Epoch 622/1000\n",
      "2335/2335 [==============================] - 0s 145us/step - loss: 16359.3356 - val_loss: 11716.9554\n",
      "Epoch 623/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 16667.5446 - val_loss: 11772.6793\n",
      "Epoch 624/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 15700.4184 - val_loss: 13284.7702\n",
      "Epoch 625/1000\n",
      "2335/2335 [==============================] - 0s 150us/step - loss: 16093.6418 - val_loss: 12328.5441\n",
      "Epoch 626/1000\n",
      "2335/2335 [==============================] - 0s 161us/step - loss: 15929.9295 - val_loss: 12541.0592\n",
      "Epoch 627/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 15729.3260 - val_loss: 12332.9387\n",
      "Epoch 628/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 15813.1111 - val_loss: 12878.3366\n",
      "Epoch 629/1000\n",
      "2335/2335 [==============================] - 0s 171us/step - loss: 16544.0543 - val_loss: 12603.3587\n",
      "Epoch 630/1000\n",
      "2335/2335 [==============================] - 0s 153us/step - loss: 16210.3730 - val_loss: 13297.6796\n",
      "Epoch 631/1000\n",
      "2335/2335 [==============================] - 0s 140us/step - loss: 15944.5130 - val_loss: 11821.6868\n",
      "Epoch 632/1000\n",
      "2335/2335 [==============================] - 0s 171us/step - loss: 15890.9754 - val_loss: 13086.3679\n",
      "Epoch 633/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 15388.7357 - val_loss: 11807.4864\n",
      "Epoch 634/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 15951.0214 - val_loss: 11746.4431\n",
      "Epoch 635/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2335/2335 [==============================] - 0s 155us/step - loss: 16157.4570 - val_loss: 12993.9887\n",
      "Epoch 636/1000\n",
      "2335/2335 [==============================] - 0s 137us/step - loss: 15488.1656 - val_loss: 11759.1456\n",
      "Epoch 637/1000\n",
      "2335/2335 [==============================] - 0s 159us/step - loss: 15737.0872 - val_loss: 11777.2278\n",
      "Epoch 638/1000\n",
      "2335/2335 [==============================] - 0s 146us/step - loss: 15692.0273 - val_loss: 13987.1195\n",
      "Epoch 639/1000\n",
      "2335/2335 [==============================] - 0s 148us/step - loss: 15768.6416 - val_loss: 12175.8052\n",
      "Epoch 640/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 16069.9469 - val_loss: 12114.8085\n",
      "Epoch 641/1000\n",
      "2335/2335 [==============================] - 0s 147us/step - loss: 15672.0536 - val_loss: 12148.5362\n",
      "Epoch 642/1000\n",
      "2335/2335 [==============================] - 0s 152us/step - loss: 15428.6909 - val_loss: 12237.7282\n",
      "Epoch 643/1000\n",
      "2335/2335 [==============================] - 0s 153us/step - loss: 15463.0874 - val_loss: 11803.9993\n",
      "Epoch 644/1000\n",
      "2335/2335 [==============================] - 0s 144us/step - loss: 16092.2707 - val_loss: 14495.5812\n",
      "Epoch 645/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 16466.5473 - val_loss: 12537.9819\n",
      "Epoch 646/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 15861.6093 - val_loss: 12082.9930\n",
      "Epoch 647/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 16300.2255 - val_loss: 12260.4873\n",
      "Epoch 648/1000\n",
      "2335/2335 [==============================] - 0s 153us/step - loss: 15884.5870 - val_loss: 13728.3661\n",
      "Epoch 649/1000\n",
      "2335/2335 [==============================] - 0s 139us/step - loss: 15944.1464 - val_loss: 12283.9472\n",
      "Epoch 650/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 15676.2556 - val_loss: 11705.6829\n",
      "Epoch 651/1000\n",
      "2335/2335 [==============================] - 0s 153us/step - loss: 15897.8149 - val_loss: 13121.7333\n",
      "Epoch 652/1000\n",
      "2335/2335 [==============================] - 0s 138us/step - loss: 15921.1115 - val_loss: 17576.7071\n",
      "Epoch 653/1000\n",
      "2335/2335 [==============================] - 0s 156us/step - loss: 15985.0833 - val_loss: 11917.3713\n",
      "Epoch 654/1000\n",
      "2335/2335 [==============================] - 0s 152us/step - loss: 16024.0547 - val_loss: 13127.5072\n",
      "Epoch 655/1000\n",
      "2335/2335 [==============================] - 0s 140us/step - loss: 16011.8908 - val_loss: 18170.5513\n",
      "Epoch 656/1000\n",
      "2335/2335 [==============================] - 0s 155us/step - loss: 15777.2987 - val_loss: 15819.0223\n",
      "Epoch 657/1000\n",
      "2335/2335 [==============================] - 0s 152us/step - loss: 16111.2974 - val_loss: 11697.5487\n",
      "Epoch 658/1000\n",
      "2335/2335 [==============================] - 0s 141us/step - loss: 15636.9010 - val_loss: 13465.6959\n",
      "Epoch 659/1000\n",
      "2335/2335 [==============================] - 0s 153us/step - loss: 15655.5314 - val_loss: 11594.0883\n",
      "Epoch 660/1000\n",
      "2335/2335 [==============================] - 0s 151us/step - loss: 15470.3971 - val_loss: 12284.1617\n",
      "Epoch 661/1000\n",
      "2335/2335 [==============================] - 0s 137us/step - loss: 15513.1594 - val_loss: 11983.0502\n",
      "Epoch 662/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 15532.9287 - val_loss: 16734.9449\n",
      "Epoch 663/1000\n",
      "2335/2335 [==============================] - 0s 164us/step - loss: 15510.2320 - val_loss: 13019.7994\n",
      "Epoch 664/1000\n",
      "2335/2335 [==============================] - 0s 151us/step - loss: 15668.9743 - val_loss: 12802.8220\n",
      "Epoch 665/1000\n",
      "2335/2335 [==============================] - 0s 140us/step - loss: 15805.3562 - val_loss: 12512.8953\n",
      "Epoch 666/1000\n",
      "2335/2335 [==============================] - 0s 171us/step - loss: 15925.4364 - val_loss: 12138.4017\n",
      "Epoch 667/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 16018.5853 - val_loss: 13537.9129\n",
      "Epoch 668/1000\n",
      "2335/2335 [==============================] - 0s 136us/step - loss: 15489.3294 - val_loss: 12631.6063\n",
      "Epoch 669/1000\n",
      "2335/2335 [==============================] - 0s 164us/step - loss: 15853.8505 - val_loss: 12312.9528\n",
      "Epoch 670/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 15925.4657 - val_loss: 14587.0451\n",
      "Epoch 671/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 15343.4341 - val_loss: 11568.0913\n",
      "Epoch 672/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 15854.5185 - val_loss: 11802.1581\n",
      "Epoch 673/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 15607.9425 - val_loss: 16112.6406\n",
      "Epoch 674/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 16521.9761 - val_loss: 12359.6329\n",
      "Epoch 675/1000\n",
      "2335/2335 [==============================] - 0s 163us/step - loss: 15890.5380 - val_loss: 11956.3711\n",
      "Epoch 676/1000\n",
      "2335/2335 [==============================] - 0s 136us/step - loss: 15408.9003 - val_loss: 11878.5916\n",
      "Epoch 677/1000\n",
      "2335/2335 [==============================] - 0s 160us/step - loss: 16201.8178 - val_loss: 12936.7365\n",
      "Epoch 678/1000\n",
      "2335/2335 [==============================] - 0s 145us/step - loss: 15891.8743 - val_loss: 12532.4911\n",
      "Epoch 679/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 15719.2968 - val_loss: 12122.6222\n",
      "Epoch 680/1000\n",
      "2335/2335 [==============================] - 0s 160us/step - loss: 15680.0607 - val_loss: 11724.9349\n",
      "Epoch 681/1000\n",
      "2335/2335 [==============================] - 0s 139us/step - loss: 15157.4525 - val_loss: 13256.6771\n",
      "Epoch 682/1000\n",
      "2335/2335 [==============================] - 0s 162us/step - loss: 15168.3989 - val_loss: 14123.4987\n",
      "Epoch 683/1000\n",
      "2335/2335 [==============================] - 0s 152us/step - loss: 15825.1360 - val_loss: 13573.5369\n",
      "Epoch 684/1000\n",
      "2335/2335 [==============================] - 0s 141us/step - loss: 15849.4826 - val_loss: 12508.4900\n",
      "Epoch 685/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 15429.3215 - val_loss: 11794.2199\n",
      "Epoch 686/1000\n",
      "2335/2335 [==============================] - 0s 151us/step - loss: 16000.5195 - val_loss: 13459.8828\n",
      "Epoch 687/1000\n",
      "2335/2335 [==============================] - 0s 141us/step - loss: 15456.5462 - val_loss: 12751.4451\n",
      "Epoch 688/1000\n",
      "2335/2335 [==============================] - 0s 165us/step - loss: 15398.0562 - val_loss: 14202.5644\n",
      "Epoch 689/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 15813.2593 - val_loss: 13523.6149\n",
      "Epoch 690/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 15371.7466 - val_loss: 12593.3309\n",
      "Epoch 691/1000\n",
      "2335/2335 [==============================] - 0s 161us/step - loss: 15839.6645 - val_loss: 12048.1311\n",
      "Epoch 692/1000\n",
      "2335/2335 [==============================] - 0s 139us/step - loss: 15746.6712 - val_loss: 16773.3900\n",
      "Epoch 693/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 15311.7313 - val_loss: 13173.3661\n",
      "Epoch 694/1000\n",
      "2335/2335 [==============================] - 0s 159us/step - loss: 15294.6248 - val_loss: 11899.2347\n",
      "Epoch 695/1000\n",
      "2335/2335 [==============================] - 0s 140us/step - loss: 14984.5818 - val_loss: 11609.3707\n",
      "Epoch 696/1000\n",
      "2335/2335 [==============================] - 0s 155us/step - loss: 15255.0519 - val_loss: 12364.7391\n",
      "Epoch 697/1000\n",
      "2335/2335 [==============================] - 0s 156us/step - loss: 15499.6599 - val_loss: 11674.9121\n",
      "Epoch 698/1000\n",
      "2335/2335 [==============================] - 0s 144us/step - loss: 15329.5234 - val_loss: 13643.3361\n",
      "Epoch 699/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 15091.6731 - val_loss: 11379.2089\n",
      "Epoch 700/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 15576.4521 - val_loss: 11823.7878\n",
      "Epoch 701/1000\n",
      "2335/2335 [==============================] - 0s 151us/step - loss: 15481.4546 - val_loss: 14975.0590\n",
      "Epoch 702/1000\n",
      "2335/2335 [==============================] - 0s 140us/step - loss: 15775.7828 - val_loss: 17379.4208\n",
      "Epoch 703/1000\n",
      "2335/2335 [==============================] - 0s 162us/step - loss: 15513.0823 - val_loss: 12243.7021\n",
      "Epoch 704/1000\n",
      "2335/2335 [==============================] - 0s 144us/step - loss: 15397.9565 - val_loss: 12349.3218\n",
      "Epoch 705/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 15578.2499 - val_loss: 19658.8207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 706/1000\n",
      "2335/2335 [==============================] - 0s 156us/step - loss: 15291.0520 - val_loss: 13223.7427\n",
      "Epoch 707/1000\n",
      "2335/2335 [==============================] - 0s 152us/step - loss: 15604.5267 - val_loss: 11578.5967\n",
      "Epoch 708/1000\n",
      "2335/2335 [==============================] - 0s 141us/step - loss: 15010.2570 - val_loss: 13235.0380\n",
      "Epoch 709/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 15369.2922 - val_loss: 12404.5600\n",
      "Epoch 710/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 15153.2968 - val_loss: 14090.9282\n",
      "Epoch 711/1000\n",
      "2335/2335 [==============================] - 0s 139us/step - loss: 15631.0108 - val_loss: 13160.9126\n",
      "Epoch 712/1000\n",
      "2335/2335 [==============================] - 0s 153us/step - loss: 14954.5099 - val_loss: 13045.4363\n",
      "Epoch 713/1000\n",
      "2335/2335 [==============================] - 0s 151us/step - loss: 15807.0839 - val_loss: 12075.6124\n",
      "Epoch 714/1000\n",
      "2335/2335 [==============================] - 0s 137us/step - loss: 15055.6444 - val_loss: 15415.7982\n",
      "Epoch 715/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 15287.0517 - val_loss: 11832.3167\n",
      "Epoch 716/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 15343.7514 - val_loss: 11736.8537\n",
      "Epoch 717/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 15814.8106 - val_loss: 12851.0747\n",
      "Epoch 718/1000\n",
      "2335/2335 [==============================] - 0s 136us/step - loss: 14783.9457 - val_loss: 11893.8103\n",
      "Epoch 719/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 15171.2725 - val_loss: 11653.7437\n",
      "Epoch 720/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 15424.5298 - val_loss: 22055.1909\n",
      "Epoch 721/1000\n",
      "2335/2335 [==============================] - 0s 135us/step - loss: 15906.7777 - val_loss: 11700.4360\n",
      "Epoch 722/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 15570.1758 - val_loss: 13249.8255\n",
      "Epoch 723/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 15009.0177 - val_loss: 14767.2459\n",
      "Epoch 724/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 15009.4673 - val_loss: 11583.5480\n",
      "Epoch 725/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 15000.4565 - val_loss: 11689.8844\n",
      "Epoch 726/1000\n",
      "2335/2335 [==============================] - 0s 152us/step - loss: 15117.5812 - val_loss: 13026.4127\n",
      "Epoch 727/1000\n",
      "2335/2335 [==============================] - 0s 140us/step - loss: 15318.4511 - val_loss: 11605.9747\n",
      "Epoch 728/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 15202.7604 - val_loss: 13787.5758\n",
      "Epoch 729/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 15499.9465 - val_loss: 12238.2522\n",
      "Epoch 730/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 15089.0245 - val_loss: 11789.3576\n",
      "Epoch 731/1000\n",
      "2335/2335 [==============================] - 0s 137us/step - loss: 14914.5231 - val_loss: 12008.6596\n",
      "Epoch 732/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 15219.1455 - val_loss: 12174.3223\n",
      "Epoch 733/1000\n",
      "2335/2335 [==============================] - 0s 165us/step - loss: 16335.6500 - val_loss: 15022.3346\n",
      "Epoch 734/1000\n",
      "2335/2335 [==============================] - 0s 155us/step - loss: 15031.7100 - val_loss: 13015.7280\n",
      "Epoch 735/1000\n",
      "2335/2335 [==============================] - 0s 150us/step - loss: 15675.4236 - val_loss: 14855.9428\n",
      "Epoch 736/1000\n",
      "2335/2335 [==============================] - 0s 150us/step - loss: 15047.9370 - val_loss: 16281.7655\n",
      "Epoch 737/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 14939.1355 - val_loss: 11910.7037\n",
      "Epoch 738/1000\n",
      "2335/2335 [==============================] - 0s 159us/step - loss: 14972.1022 - val_loss: 12446.9925\n",
      "Epoch 739/1000\n",
      "2335/2335 [==============================] - 0s 140us/step - loss: 15353.3384 - val_loss: 12286.8527\n",
      "Epoch 740/1000\n",
      "2335/2335 [==============================] - 0s 152us/step - loss: 15158.9435 - val_loss: 12429.5431\n",
      "Epoch 741/1000\n",
      "2335/2335 [==============================] - 0s 152us/step - loss: 15084.8226 - val_loss: 11777.2089\n",
      "Epoch 742/1000\n",
      "2335/2335 [==============================] - 0s 137us/step - loss: 15405.0481 - val_loss: 11987.0554\n",
      "Epoch 743/1000\n",
      "2335/2335 [==============================] - 0s 161us/step - loss: 15138.9389 - val_loss: 12187.7421\n",
      "Epoch 744/1000\n",
      "2335/2335 [==============================] - 0s 153us/step - loss: 14913.1289 - val_loss: 12219.7897\n",
      "Epoch 745/1000\n",
      "2335/2335 [==============================] - 0s 135us/step - loss: 15079.5145 - val_loss: 12934.0307\n",
      "Epoch 746/1000\n",
      "2335/2335 [==============================] - 0s 161us/step - loss: 15701.0604 - val_loss: 12641.9676\n",
      "Epoch 747/1000\n",
      "2335/2335 [==============================] - 0s 153us/step - loss: 14430.4329 - val_loss: 11880.4573\n",
      "Epoch 748/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 15150.4584 - val_loss: 12900.5951\n",
      "Epoch 749/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 14891.1438 - val_loss: 13545.3600\n",
      "Epoch 750/1000\n",
      "2335/2335 [==============================] - 0s 151us/step - loss: 15506.7985 - val_loss: 12064.0534\n",
      "Epoch 751/1000\n",
      "2335/2335 [==============================] - 0s 141us/step - loss: 14943.8389 - val_loss: 11877.1054\n",
      "Epoch 752/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 15155.5548 - val_loss: 13609.7338\n",
      "Epoch 753/1000\n",
      "2335/2335 [==============================] - 0s 156us/step - loss: 15110.5978 - val_loss: 13488.4560\n",
      "Epoch 754/1000\n",
      "2335/2335 [==============================] - 0s 150us/step - loss: 14918.3066 - val_loss: 12885.5602\n",
      "Epoch 755/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 14946.7719 - val_loss: 11959.4177\n",
      "Epoch 756/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 15182.8648 - val_loss: 15993.4794\n",
      "Epoch 757/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 14635.8441 - val_loss: 14188.0001\n",
      "Epoch 758/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 14392.3682 - val_loss: 14116.1395\n",
      "Epoch 759/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 14871.8627 - val_loss: 12797.6149\n",
      "Epoch 760/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 14929.2086 - val_loss: 11632.9674\n",
      "Epoch 761/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 15099.0494 - val_loss: 11543.5384\n",
      "Epoch 762/1000\n",
      "2335/2335 [==============================] - 0s 164us/step - loss: 14750.5614 - val_loss: 13981.6177\n",
      "Epoch 763/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 14778.7776 - val_loss: 11975.3299\n",
      "Epoch 764/1000\n",
      "2335/2335 [==============================] - 0s 148us/step - loss: 15035.1958 - val_loss: 16242.7086\n",
      "Epoch 765/1000\n",
      "2335/2335 [==============================] - 0s 156us/step - loss: 15122.1127 - val_loss: 14162.7035\n",
      "Epoch 766/1000\n",
      "2335/2335 [==============================] - 0s 137us/step - loss: 14865.4073 - val_loss: 14036.6588\n",
      "Epoch 767/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 14482.9548 - val_loss: 13959.1391\n",
      "Epoch 768/1000\n",
      "2335/2335 [==============================] - 0s 160us/step - loss: 15124.4487 - val_loss: 12976.0020\n",
      "Epoch 769/1000\n",
      "2335/2335 [==============================] - 0s 139us/step - loss: 14653.5150 - val_loss: 11841.7978\n",
      "Epoch 770/1000\n",
      "2335/2335 [==============================] - 0s 153us/step - loss: 15072.6803 - val_loss: 11931.3957\n",
      "Epoch 771/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 14955.2679 - val_loss: 13118.4794\n",
      "Epoch 772/1000\n",
      "2335/2335 [==============================] - 0s 141us/step - loss: 14564.4462 - val_loss: 12360.8470\n",
      "Epoch 773/1000\n",
      "2335/2335 [==============================] - 0s 150us/step - loss: 14980.5239 - val_loss: 14616.2266\n",
      "Epoch 774/1000\n",
      "2335/2335 [==============================] - 0s 152us/step - loss: 14457.9034 - val_loss: 11768.9686\n",
      "Epoch 775/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 14999.0409 - val_loss: 12402.8958\n",
      "Epoch 776/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2335/2335 [==============================] - 0s 141us/step - loss: 14837.1467 - val_loss: 12736.1071\n",
      "Epoch 777/1000\n",
      "2335/2335 [==============================] - 0s 153us/step - loss: 14772.6892 - val_loss: 12397.7169\n",
      "Epoch 778/1000\n",
      "2335/2335 [==============================] - 0s 139us/step - loss: 14454.3329 - val_loss: 13936.3620\n",
      "Epoch 779/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 14697.8275 - val_loss: 11745.3628\n",
      "Epoch 780/1000\n",
      "2335/2335 [==============================] - 0s 160us/step - loss: 15037.0995 - val_loss: 12449.6690\n",
      "Epoch 781/1000\n",
      "2335/2335 [==============================] - 0s 145us/step - loss: 14442.7660 - val_loss: 11845.5881\n",
      "Epoch 782/1000\n",
      "2335/2335 [==============================] - 0s 137us/step - loss: 15043.2891 - val_loss: 12438.0957\n",
      "Epoch 783/1000\n",
      "2335/2335 [==============================] - 0s 150us/step - loss: 14935.5646 - val_loss: 12914.3916\n",
      "Epoch 784/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 14053.7362 - val_loss: 11696.6022\n",
      "Epoch 785/1000\n",
      "2335/2335 [==============================] - 0s 139us/step - loss: 14852.1527 - val_loss: 13194.4251\n",
      "Epoch 786/1000\n",
      "2335/2335 [==============================] - 0s 152us/step - loss: 15043.7200 - val_loss: 13014.0287\n",
      "Epoch 787/1000\n",
      "2335/2335 [==============================] - 0s 152us/step - loss: 14985.3201 - val_loss: 13099.3924\n",
      "Epoch 788/1000\n",
      "2335/2335 [==============================] - 0s 156us/step - loss: 14504.5315 - val_loss: 13362.3228\n",
      "Epoch 789/1000\n",
      "2335/2335 [==============================] - 0s 137us/step - loss: 14747.5208 - val_loss: 11794.0418\n",
      "Epoch 790/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 14778.4173 - val_loss: 12395.4690\n",
      "Epoch 791/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 14586.7463 - val_loss: 12339.3746\n",
      "Epoch 792/1000\n",
      "2335/2335 [==============================] - 0s 152us/step - loss: 14063.7421 - val_loss: 11927.9343\n",
      "Epoch 793/1000\n",
      "2335/2335 [==============================] - 0s 150us/step - loss: 14581.4808 - val_loss: 11787.4429\n",
      "Epoch 794/1000\n",
      "2335/2335 [==============================] - 0s 153us/step - loss: 14986.3031 - val_loss: 13160.3360\n",
      "Epoch 795/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 14674.5600 - val_loss: 11917.5663\n",
      "Epoch 796/1000\n",
      "2335/2335 [==============================] - 0s 150us/step - loss: 14347.2038 - val_loss: 12407.9185\n",
      "Epoch 797/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 14388.2079 - val_loss: 12819.0124\n",
      "Epoch 798/1000\n",
      "2335/2335 [==============================] - 0s 162us/step - loss: 14425.8762 - val_loss: 12463.8411\n",
      "Epoch 799/1000\n",
      "2335/2335 [==============================] - 0s 144us/step - loss: 14616.7329 - val_loss: 12581.7687\n",
      "Epoch 800/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 14366.5633 - val_loss: 12027.9158\n",
      "Epoch 801/1000\n",
      "2335/2335 [==============================] - 0s 165us/step - loss: 14786.3775 - val_loss: 13471.1668\n",
      "Epoch 802/1000\n",
      "2335/2335 [==============================] - 0s 150us/step - loss: 14891.7725 - val_loss: 14416.5040\n",
      "Epoch 803/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 14827.2812 - val_loss: 11756.5394\n",
      "Epoch 804/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 14333.6052 - val_loss: 12938.3732\n",
      "Epoch 805/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 15124.9478 - val_loss: 13753.0927\n",
      "Epoch 806/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 14792.9435 - val_loss: 12981.0908\n",
      "Epoch 807/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 14900.0019 - val_loss: 13973.2728\n",
      "Epoch 808/1000\n",
      "2335/2335 [==============================] - 0s 147us/step - loss: 15022.3663 - val_loss: 11947.0057\n",
      "Epoch 809/1000\n",
      "2335/2335 [==============================] - 0s 166us/step - loss: 14899.2049 - val_loss: 15187.0122\n",
      "Epoch 810/1000\n",
      "2335/2335 [==============================] - 0s 159us/step - loss: 15255.3059 - val_loss: 12911.9355\n",
      "Epoch 811/1000\n",
      "2335/2335 [==============================] - 0s 153us/step - loss: 14757.4800 - val_loss: 12272.1543\n",
      "Epoch 812/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 14511.0995 - val_loss: 12045.1690\n",
      "Epoch 813/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 15412.8121 - val_loss: 16537.7870\n",
      "Epoch 814/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 14691.3381 - val_loss: 12476.5695\n",
      "Epoch 815/1000\n",
      "2335/2335 [==============================] - 0s 150us/step - loss: 15174.2557 - val_loss: 12187.8456\n",
      "Epoch 816/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 14679.7730 - val_loss: 13747.9427\n",
      "Epoch 817/1000\n",
      "2335/2335 [==============================] - 0s 150us/step - loss: 14597.2022 - val_loss: 12138.6231\n",
      "Epoch 818/1000\n",
      "2335/2335 [==============================] - 0s 171us/step - loss: 14521.4545 - val_loss: 12933.1451\n",
      "Epoch 819/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 14601.3851 - val_loss: 12054.8642\n",
      "Epoch 820/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 14894.3128 - val_loss: 12131.1351\n",
      "Epoch 821/1000\n",
      "2335/2335 [==============================] - 0s 175us/step - loss: 14844.3177 - val_loss: 11897.1969\n",
      "Epoch 822/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 14582.4869 - val_loss: 11954.4056\n",
      "Epoch 823/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 13945.9133 - val_loss: 13719.4922\n",
      "Epoch 824/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 14470.2379 - val_loss: 12533.6594\n",
      "Epoch 825/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 14698.7976 - val_loss: 12431.2817\n",
      "Epoch 826/1000\n",
      "2335/2335 [==============================] - 0s 137us/step - loss: 14266.0435 - val_loss: 11906.9521\n",
      "Epoch 827/1000\n",
      "2335/2335 [==============================] - 0s 177us/step - loss: 14719.7219 - val_loss: 14963.3655\n",
      "Epoch 828/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 14982.5352 - val_loss: 14321.1927\n",
      "Epoch 829/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 14468.9758 - val_loss: 18406.9490\n",
      "Epoch 830/1000\n",
      "2335/2335 [==============================] - 0s 186us/step - loss: 14628.3051 - val_loss: 11584.6006\n",
      "Epoch 831/1000\n",
      "2335/2335 [==============================] - 0s 164us/step - loss: 14197.6448 - val_loss: 12602.9494\n",
      "Epoch 832/1000\n",
      "2335/2335 [==============================] - 0s 165us/step - loss: 14931.1259 - val_loss: 15319.3762\n",
      "Epoch 833/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 14575.4508 - val_loss: 11577.3429\n",
      "Epoch 834/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 14939.2772 - val_loss: 14187.7978\n",
      "Epoch 835/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 14331.2053 - val_loss: 16269.1814\n",
      "Epoch 836/1000\n",
      "2335/2335 [==============================] - 0s 180us/step - loss: 14730.5495 - val_loss: 13331.9369\n",
      "Epoch 837/1000\n",
      "2335/2335 [==============================] - 0s 140us/step - loss: 14305.6270 - val_loss: 14065.4019\n",
      "Epoch 838/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 14749.0355 - val_loss: 14750.3820\n",
      "Epoch 839/1000\n",
      "2335/2335 [==============================] - 0s 177us/step - loss: 15041.0456 - val_loss: 13521.1411\n",
      "Epoch 840/1000\n",
      "2335/2335 [==============================] - 0s 155us/step - loss: 13992.6554 - val_loss: 11941.0281\n",
      "Epoch 841/1000\n",
      "2335/2335 [==============================] - 0s 174us/step - loss: 14678.3115 - val_loss: 13063.8210\n",
      "Epoch 842/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 14527.2519 - val_loss: 11879.4277\n",
      "Epoch 843/1000\n",
      "2335/2335 [==============================] - 0s 175us/step - loss: 14763.9816 - val_loss: 15431.7322\n",
      "Epoch 844/1000\n",
      "2335/2335 [==============================] - 0s 159us/step - loss: 14704.3933 - val_loss: 11872.0285\n",
      "Epoch 845/1000\n",
      "2335/2335 [==============================] - 0s 165us/step - loss: 14390.8447 - val_loss: 12244.0664\n",
      "Epoch 846/1000\n",
      "2335/2335 [==============================] - 0s 163us/step - loss: 14081.6954 - val_loss: 11818.2567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 847/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 14377.1753 - val_loss: 13100.7781\n",
      "Epoch 848/1000\n",
      "2335/2335 [==============================] - 0s 153us/step - loss: 14397.8502 - val_loss: 15051.5986\n",
      "Epoch 849/1000\n",
      "2335/2335 [==============================] - 0s 138us/step - loss: 14410.4359 - val_loss: 12430.8279\n",
      "Epoch 850/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 14263.4636 - val_loss: 12227.8370\n",
      "Epoch 851/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 14526.5383 - val_loss: 12169.6538\n",
      "Epoch 852/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 14148.6990 - val_loss: 12256.8855\n",
      "Epoch 853/1000\n",
      "2335/2335 [==============================] - 0s 156us/step - loss: 14793.6046 - val_loss: 12480.9570\n",
      "Epoch 854/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 13986.1963 - val_loss: 11633.6933\n",
      "Epoch 855/1000\n",
      "2335/2335 [==============================] - 0s 151us/step - loss: 13977.1631 - val_loss: 13607.7669\n",
      "Epoch 856/1000\n",
      "2335/2335 [==============================] - 0s 155us/step - loss: 14382.9908 - val_loss: 12077.7179\n",
      "Epoch 857/1000\n",
      "2335/2335 [==============================] - 0s 155us/step - loss: 14162.1801 - val_loss: 12701.8617\n",
      "Epoch 858/1000\n",
      "2335/2335 [==============================] - 0s 137us/step - loss: 15435.8293 - val_loss: 14110.5124\n",
      "Epoch 859/1000\n",
      "2335/2335 [==============================] - 0s 159us/step - loss: 14722.8017 - val_loss: 12079.4812\n",
      "Epoch 860/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 14142.4573 - val_loss: 17962.3169\n",
      "Epoch 861/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 14156.9040 - val_loss: 12765.6981\n",
      "Epoch 862/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 13980.4470 - val_loss: 12284.5970\n",
      "Epoch 863/1000\n",
      "2335/2335 [==============================] - 0s 159us/step - loss: 14068.5479 - val_loss: 11939.6184\n",
      "Epoch 864/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 14402.4494 - val_loss: 14760.4438\n",
      "Epoch 865/1000\n",
      "2335/2335 [==============================] - 0s 138us/step - loss: 14710.8249 - val_loss: 17700.4437\n",
      "Epoch 866/1000\n",
      "2335/2335 [==============================] - 0s 171us/step - loss: 14212.6174 - val_loss: 13519.4364\n",
      "Epoch 867/1000\n",
      "2335/2335 [==============================] - 0s 156us/step - loss: 14351.8965 - val_loss: 14164.5409\n",
      "Epoch 868/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 14446.3718 - val_loss: 13533.1362\n",
      "Epoch 869/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 14297.0646 - val_loss: 12471.4836\n",
      "Epoch 870/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 14206.2362 - val_loss: 12481.8356\n",
      "Epoch 871/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 14188.1179 - val_loss: 13646.7163\n",
      "Epoch 872/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 14307.4854 - val_loss: 14917.5026\n",
      "Epoch 873/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 14230.3490 - val_loss: 12726.0705\n",
      "Epoch 874/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 14336.4255 - val_loss: 12097.6505\n",
      "Epoch 875/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 14225.4323 - val_loss: 11814.1307\n",
      "Epoch 876/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 14085.4676 - val_loss: 12674.7685\n",
      "Epoch 877/1000\n",
      "2335/2335 [==============================] - 0s 162us/step - loss: 14886.3992 - val_loss: 13286.8120\n",
      "Epoch 878/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 14543.2153 - val_loss: 12054.7180\n",
      "Epoch 879/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 13854.0531 - val_loss: 12604.5787\n",
      "Epoch 880/1000\n",
      "2335/2335 [==============================] - 0s 156us/step - loss: 14546.8072 - val_loss: 12378.8464\n",
      "Epoch 881/1000\n",
      "2335/2335 [==============================] - 0s 155us/step - loss: 14624.0372 - val_loss: 13226.8625\n",
      "Epoch 882/1000\n",
      "2335/2335 [==============================] - 0s 138us/step - loss: 13831.9965 - val_loss: 12005.3535\n",
      "Epoch 883/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 14057.7548 - val_loss: 13190.1676\n",
      "Epoch 884/1000\n",
      "2335/2335 [==============================] - 0s 164us/step - loss: 14014.2942 - val_loss: 14026.4911\n",
      "Epoch 885/1000\n",
      "2335/2335 [==============================] - 0s 148us/step - loss: 14114.9531 - val_loss: 12087.4582\n",
      "Epoch 886/1000\n",
      "2335/2335 [==============================] - 0s 144us/step - loss: 14146.5562 - val_loss: 12489.0099\n",
      "Epoch 887/1000\n",
      "2335/2335 [==============================] - 0s 164us/step - loss: 14369.9599 - val_loss: 11564.3041\n",
      "Epoch 888/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 14145.4542 - val_loss: 12221.6148\n",
      "Epoch 889/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 14091.0701 - val_loss: 13418.3443\n",
      "Epoch 890/1000\n",
      "2335/2335 [==============================] - 0s 164us/step - loss: 13652.4594 - val_loss: 11838.4124\n",
      "Epoch 891/1000\n",
      "2335/2335 [==============================] - 0s 175us/step - loss: 14745.5349 - val_loss: 12234.9228\n",
      "Epoch 892/1000\n",
      "2335/2335 [==============================] - 0s 159us/step - loss: 13992.8740 - val_loss: 12353.1690\n",
      "Epoch 893/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 14671.3673 - val_loss: 11900.8973\n",
      "Epoch 894/1000\n",
      "2335/2335 [==============================] - 0s 185us/step - loss: 14383.6169 - val_loss: 12586.9175\n",
      "Epoch 895/1000\n",
      "2335/2335 [==============================] - 0s 179us/step - loss: 14419.3304 - val_loss: 13110.0624\n",
      "Epoch 896/1000\n",
      "2335/2335 [==============================] - 0s 199us/step - loss: 14477.6390 - val_loss: 11775.6318\n",
      "Epoch 897/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 14720.2390 - val_loss: 12156.5145\n",
      "Epoch 898/1000\n",
      "2335/2335 [==============================] - 0s 162us/step - loss: 13505.7069 - val_loss: 12451.6432\n",
      "Epoch 899/1000\n",
      "2335/2335 [==============================] - 0s 172us/step - loss: 14252.2517 - val_loss: 12255.2966\n",
      "Epoch 900/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 13924.6683 - val_loss: 13942.6167\n",
      "Epoch 901/1000\n",
      "2335/2335 [==============================] - 0s 171us/step - loss: 14154.5297 - val_loss: 13178.6297\n",
      "Epoch 902/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 13951.0652 - val_loss: 12192.2181\n",
      "Epoch 903/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 13728.9052 - val_loss: 12300.5145\n",
      "Epoch 904/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 13954.1862 - val_loss: 11922.3521\n",
      "Epoch 905/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 14581.2617 - val_loss: 12113.1723\n",
      "Epoch 906/1000\n",
      "2335/2335 [==============================] - 0s 164us/step - loss: 14249.2118 - val_loss: 15560.0065\n",
      "Epoch 907/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 14341.8834 - val_loss: 12241.5606\n",
      "Epoch 908/1000\n",
      "2335/2335 [==============================] - 0s 152us/step - loss: 13806.0099 - val_loss: 13940.0267\n",
      "Epoch 909/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 14207.6917 - val_loss: 12565.5114\n",
      "Epoch 910/1000\n",
      "2335/2335 [==============================] - 0s 151us/step - loss: 14109.7335 - val_loss: 12276.2264\n",
      "Epoch 911/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 13703.2052 - val_loss: 12083.7013\n",
      "Epoch 912/1000\n",
      "2335/2335 [==============================] - 0s 139us/step - loss: 13947.8812 - val_loss: 12667.0249\n",
      "Epoch 913/1000\n",
      "2335/2335 [==============================] - 0s 163us/step - loss: 13767.7535 - val_loss: 12560.8818\n",
      "Epoch 914/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 14110.1870 - val_loss: 13014.3579\n",
      "Epoch 915/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 13788.2607 - val_loss: 12045.4747\n",
      "Epoch 916/1000\n",
      "2335/2335 [==============================] - 0s 164us/step - loss: 13739.2451 - val_loss: 11912.1560\n",
      "Epoch 917/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2335/2335 [==============================] - 0s 157us/step - loss: 13896.0421 - val_loss: 13098.7957\n",
      "Epoch 918/1000\n",
      "2335/2335 [==============================] - 0s 136us/step - loss: 14282.4115 - val_loss: 16813.1430\n",
      "Epoch 919/1000\n",
      "2335/2335 [==============================] - 0s 164us/step - loss: 13790.6988 - val_loss: 14511.3324\n",
      "Epoch 920/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 13762.6831 - val_loss: 13791.3197\n",
      "Epoch 921/1000\n",
      "2335/2335 [==============================] - 0s 136us/step - loss: 13695.7404 - val_loss: 13639.7474\n",
      "Epoch 922/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 14603.2302 - val_loss: 11916.6291\n",
      "Epoch 923/1000\n",
      "2335/2335 [==============================] - 0s 150us/step - loss: 13761.1678 - val_loss: 13244.2302\n",
      "Epoch 924/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 13705.0686 - val_loss: 11779.4797\n",
      "Epoch 925/1000\n",
      "2335/2335 [==============================] - 0s 141us/step - loss: 13732.9889 - val_loss: 16852.2318\n",
      "Epoch 926/1000\n",
      "2335/2335 [==============================] - 0s 151us/step - loss: 14344.5948 - val_loss: 12617.3464\n",
      "Epoch 927/1000\n",
      "2335/2335 [==============================] - 0s 156us/step - loss: 13790.1950 - val_loss: 15181.2321\n",
      "Epoch 928/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 13842.5656 - val_loss: 13453.5169\n",
      "Epoch 929/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 14434.2523 - val_loss: 11972.8516\n",
      "Epoch 930/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 13948.6107 - val_loss: 14548.3272\n",
      "Epoch 931/1000\n",
      "2335/2335 [==============================] - 0s 136us/step - loss: 13778.2970 - val_loss: 11856.8201\n",
      "Epoch 932/1000\n",
      "2335/2335 [==============================] - 0s 159us/step - loss: 13678.1042 - val_loss: 12868.4172\n",
      "Epoch 933/1000\n",
      "2335/2335 [==============================] - 0s 140us/step - loss: 14884.4765 - val_loss: 14739.2133\n",
      "Epoch 934/1000\n",
      "2335/2335 [==============================] - 0s 136us/step - loss: 14095.6754 - val_loss: 12854.0203\n",
      "Epoch 935/1000\n",
      "2335/2335 [==============================] - 0s 164us/step - loss: 14114.1016 - val_loss: 12350.0179\n",
      "Epoch 936/1000\n",
      "2335/2335 [==============================] - 0s 152us/step - loss: 14353.6733 - val_loss: 14100.8011\n",
      "Epoch 937/1000\n",
      "2335/2335 [==============================] - 0s 140us/step - loss: 13669.5274 - val_loss: 14587.4501\n",
      "Epoch 938/1000\n",
      "2335/2335 [==============================] - 0s 164us/step - loss: 14039.0611 - val_loss: 13398.5416\n",
      "Epoch 939/1000\n",
      "2335/2335 [==============================] - 0s 148us/step - loss: 14424.4344 - val_loss: 17114.6655\n",
      "Epoch 940/1000\n",
      "2335/2335 [==============================] - 0s 151us/step - loss: 14166.1043 - val_loss: 11965.7889\n",
      "Epoch 941/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 14045.2454 - val_loss: 12474.9723\n",
      "Epoch 942/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 14043.3262 - val_loss: 11878.2050\n",
      "Epoch 943/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 13845.0368 - val_loss: 12257.7490\n",
      "Epoch 944/1000\n",
      "2335/2335 [==============================] - 0s 145us/step - loss: 13925.3135 - val_loss: 11976.8017\n",
      "Epoch 945/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 14171.4458 - val_loss: 12195.4244\n",
      "Epoch 946/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 13748.5637 - val_loss: 12345.6042\n",
      "Epoch 947/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 13601.7750 - val_loss: 13525.1189\n",
      "Epoch 948/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 13793.4324 - val_loss: 12835.3311\n",
      "Epoch 949/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 13982.1443 - val_loss: 14071.4747\n",
      "Epoch 950/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 14045.1387 - val_loss: 15964.4174\n",
      "Epoch 951/1000\n",
      "2335/2335 [==============================] - 0s 167us/step - loss: 14603.3935 - val_loss: 13480.4747\n",
      "Epoch 952/1000\n",
      "2335/2335 [==============================] - 0s 140us/step - loss: 13759.2891 - val_loss: 12379.8588\n",
      "Epoch 953/1000\n",
      "2335/2335 [==============================] - 0s 159us/step - loss: 13950.2138 - val_loss: 13217.8928\n",
      "Epoch 954/1000\n",
      "2335/2335 [==============================] - 0s 153us/step - loss: 13927.1341 - val_loss: 12687.6207\n",
      "Epoch 955/1000\n",
      "2335/2335 [==============================] - 0s 136us/step - loss: 14192.3372 - val_loss: 14259.1246\n",
      "Epoch 956/1000\n",
      "2335/2335 [==============================] - 0s 155us/step - loss: 14021.1476 - val_loss: 12432.0233\n",
      "Epoch 957/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 13920.5357 - val_loss: 12413.6205\n",
      "Epoch 958/1000\n",
      "2335/2335 [==============================] - 0s 159us/step - loss: 13795.3162 - val_loss: 11895.9089\n",
      "Epoch 959/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 14592.1245 - val_loss: 11883.5117\n",
      "Epoch 960/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 14274.9756 - val_loss: 12108.8651\n",
      "Epoch 961/1000\n",
      "2335/2335 [==============================] - 0s 156us/step - loss: 13697.0593 - val_loss: 12344.9686\n",
      "Epoch 962/1000\n",
      "2335/2335 [==============================] - 0s 136us/step - loss: 13992.7715 - val_loss: 12487.9466\n",
      "Epoch 963/1000\n",
      "2335/2335 [==============================] - 0s 151us/step - loss: 14030.1450 - val_loss: 12329.7636\n",
      "Epoch 964/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 13901.5304 - val_loss: 12113.9479\n",
      "Epoch 965/1000\n",
      "2335/2335 [==============================] - 0s 144us/step - loss: 13698.8598 - val_loss: 12456.6610\n",
      "Epoch 966/1000\n",
      "2335/2335 [==============================] - 0s 150us/step - loss: 14109.6619 - val_loss: 14442.8595\n",
      "Epoch 967/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 13968.2430 - val_loss: 12050.9865\n",
      "Epoch 968/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 13503.2557 - val_loss: 12566.3765\n",
      "Epoch 969/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 14117.0069 - val_loss: 12211.4616\n",
      "Epoch 970/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 14081.4040 - val_loss: 18268.4214\n",
      "Epoch 971/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 13798.1896 - val_loss: 12518.3232\n",
      "Epoch 972/1000\n",
      "2335/2335 [==============================] - 0s 164us/step - loss: 14393.1078 - val_loss: 14068.7261\n",
      "Epoch 973/1000\n",
      "2335/2335 [==============================] - 0s 148us/step - loss: 14080.8462 - val_loss: 14252.0108\n",
      "Epoch 974/1000\n",
      "2335/2335 [==============================] - 0s 137us/step - loss: 14201.0026 - val_loss: 14541.7911\n",
      "Epoch 975/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 13780.4620 - val_loss: 16204.5596\n",
      "Epoch 976/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 14256.9524 - val_loss: 12053.6513\n",
      "Epoch 977/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 14065.3806 - val_loss: 12567.4382\n",
      "Epoch 978/1000\n",
      "2335/2335 [==============================] - 0s 161us/step - loss: 13326.0008 - val_loss: 12226.5356\n",
      "Epoch 979/1000\n",
      "2335/2335 [==============================] - 0s 152us/step - loss: 13741.5187 - val_loss: 13671.3349\n",
      "Epoch 980/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 13967.3444 - val_loss: 12694.2114\n",
      "Epoch 981/1000\n",
      "2335/2335 [==============================] - 0s 163us/step - loss: 13544.1125 - val_loss: 12357.2354\n",
      "Epoch 982/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 14121.7525 - val_loss: 16048.4662\n",
      "Epoch 983/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 14073.3432 - val_loss: 13421.4297\n",
      "Epoch 984/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 14100.1001 - val_loss: 13022.5129\n",
      "Epoch 985/1000\n",
      "2335/2335 [==============================] - 0s 154us/step - loss: 13517.7633 - val_loss: 12333.0546\n",
      "Epoch 986/1000\n",
      "2335/2335 [==============================] - 0s 138us/step - loss: 13489.0380 - val_loss: 16182.3255\n",
      "Epoch 987/1000\n",
      "2335/2335 [==============================] - 0s 164us/step - loss: 14018.7911 - val_loss: 14665.4212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 988/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 13589.4215 - val_loss: 12458.3727\n",
      "Epoch 989/1000\n",
      "2335/2335 [==============================] - 0s 136us/step - loss: 13987.5981 - val_loss: 13296.7751\n",
      "Epoch 990/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 13775.5600 - val_loss: 14160.4626\n",
      "Epoch 991/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 14254.8375 - val_loss: 12975.2097\n",
      "Epoch 992/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 13792.3917 - val_loss: 12074.0515\n",
      "Epoch 993/1000\n",
      "2335/2335 [==============================] - 0s 158us/step - loss: 13289.3744 - val_loss: 12341.8748\n",
      "Epoch 994/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 13967.1518 - val_loss: 13163.5748\n",
      "Epoch 995/1000\n",
      "2335/2335 [==============================] - 0s 157us/step - loss: 14214.5095 - val_loss: 11773.9065\n",
      "Epoch 996/1000\n",
      "2335/2335 [==============================] - 0s 142us/step - loss: 13739.9561 - val_loss: 13092.4520\n",
      "Epoch 997/1000\n",
      "2335/2335 [==============================] - 0s 143us/step - loss: 14153.9516 - val_loss: 12577.5757\n",
      "Epoch 998/1000\n",
      "2335/2335 [==============================] - 0s 136us/step - loss: 14048.4135 - val_loss: 14159.8971\n",
      "Epoch 999/1000\n",
      "2335/2335 [==============================] - 0s 149us/step - loss: 14029.3643 - val_loss: 12293.6743\n",
      "Epoch 1000/1000\n",
      "2335/2335 [==============================] - 0s 164us/step - loss: 14278.5586 - val_loss: 13841.9345\n"
     ]
    }
   ],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LeakyReLU,PReLU,ELU\n",
    "from keras.layers import Dropout\n",
    "\n",
    "\n",
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(output_dim = 50, init = 'he_uniform',activation='relu',input_dim = 177))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(output_dim = 25, init = 'he_uniform',activation='relu'))\n",
    "\n",
    "# Adding the third hidden layer\n",
    "classifier.add(Dense(output_dim = 50, init = 'he_uniform',activation='relu'))\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(output_dim = 1, init = 'he_uniform'))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(loss=root_mean_squared_error, optimizer='Adamax')\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "model_history=classifier.fit(X_train.values, y_train.values,validation_split=0.20, batch_size = 10, nb_epoch = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_pred=classifier.predict(df_Test.drop(['SalePrice'],axis=1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create Sample Submission file and Submit using ANN\n",
    "pred=pd.DataFrame(ann_pred)\n",
    "sub_df=pd.read_csv('sample_submission.csv')\n",
    "datasets=pd.concat([sub_df['Id'],pred],axis=1)\n",
    "datasets.columns=['Id','SalePrice']\n",
    "datasets.to_csv('sample_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'finalized_model2.pkl'\n",
    "pickle.dump(model_history, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
